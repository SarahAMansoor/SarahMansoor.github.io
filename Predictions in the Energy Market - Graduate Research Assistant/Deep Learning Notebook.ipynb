{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c95ef7",
   "metadata": {},
   "source": [
    "# Deep Learning Notebook\n",
    "### MGTA 611 Business Applications of Artificial Intelligence\n",
    "### Sarah Mansoor\n",
    "### March 27, 2023\n",
    "\n",
    "### Table of Contents\n",
    "* Data preparation\n",
    "* Simple Neural Network\n",
    "* Recurrent Neural Network (RNN)\n",
    "* LSTM Neural Network\n",
    "* Convolutional Neural Network (CNN)\n",
    "* Traditional vs Deep Learning solutions\n",
    "\n",
    "The notebook starts with preparing the data, which involves loading the MISO offers data from Big Query into the notebook and cleaning the data by renaming columns, removing duplicates and outliers. Once the data is cleaned, it is split into training and test sets before being passed into various models. The models used for training include a simple neural network, a recurrent neural network, LSTM neural network, and a convolutional neural network. Finally, the notebook concludes with a comparison of traditional data science methods and deep learning solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6434d",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Loading Data from Big Query to Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63724a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import bigquery from the google cloud library\n",
    "# Import service account from the google oauth2 library\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51744e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize credentials using the service account json key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    'misoelect-16349cd8bba4.json')\n",
    "project_id = 'misoelect'\n",
    "# Initilize the client in big query with the following credentials and project ID\n",
    "client = bigquery.Client(credentials= credentials, project=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ba9a0c",
   "metadata": {},
   "source": [
    "#### MISO Offers Table\n",
    "\n",
    "The first step is to load the MISO Offers table from big query. I limited the number of rows to 1,555,000 as the models would take a long time to run with all the values from 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ebeda570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the miso_offers table from the misodb in misoelect's bigquery\n",
    "miso_offers = \"misoelect.misodb.miso_offers\"\n",
    "# Query for the table using only the year 2016\n",
    "query_job = client.query(\"\"\"\n",
    "SELECT * \n",
    "FROM `misoelect.misodb.miso_offers`\n",
    "WHERE\n",
    "((Region = 'Central') OR (Region = 'South') OR (Region = 'North'))\n",
    "AND\n",
    "(BeginningTimeEST BETWEEN '2016-01-01 00:00:00' AND '2016-12-31 12:00:00')\n",
    "LIMIT 1555000\n",
    "\"\"\")\n",
    "# Wait for the job to complete.\n",
    "results = query_job.result() \n",
    "# Create dataframe from results\n",
    "miso_offers_df = results.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3077abed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>OwnerCode</th>\n",
       "      <th>UnitCode</th>\n",
       "      <th>UnitType</th>\n",
       "      <th>BeginningTimeEST</th>\n",
       "      <th>EndTimeEST</th>\n",
       "      <th>EconomicMax</th>\n",
       "      <th>EconomicMin</th>\n",
       "      <th>EmergencyMax</th>\n",
       "      <th>EmergencyMin</th>\n",
       "      <th>...</th>\n",
       "      <th>MW6</th>\n",
       "      <th>Price7</th>\n",
       "      <th>MW7</th>\n",
       "      <th>Price8</th>\n",
       "      <th>MW8</th>\n",
       "      <th>Price9</th>\n",
       "      <th>MW9</th>\n",
       "      <th>Price10</th>\n",
       "      <th>MW10</th>\n",
       "      <th>Slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Central</td>\n",
       "      <td>122062517</td>\n",
       "      <td>2968</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-06-03 01:00:00</td>\n",
       "      <td>2016-06-03 02:00:00</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>373.0</td>\n",
       "      <td>22.62</td>\n",
       "      <td>411.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>448.0</td>\n",
       "      <td>23.49</td>\n",
       "      <td>501.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Central</td>\n",
       "      <td>122062517</td>\n",
       "      <td>2968</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-06-03 03:00:00</td>\n",
       "      <td>2016-06-03 04:00:00</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>373.0</td>\n",
       "      <td>22.62</td>\n",
       "      <td>411.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>448.0</td>\n",
       "      <td>23.49</td>\n",
       "      <td>501.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Central</td>\n",
       "      <td>122062517</td>\n",
       "      <td>2968</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-06-03 10:00:00</td>\n",
       "      <td>2016-06-03 11:00:00</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>373.0</td>\n",
       "      <td>22.62</td>\n",
       "      <td>411.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>448.0</td>\n",
       "      <td>23.49</td>\n",
       "      <td>501.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Central</td>\n",
       "      <td>122062517</td>\n",
       "      <td>2968</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-06-03 12:00:00</td>\n",
       "      <td>2016-06-03 13:00:00</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>373.0</td>\n",
       "      <td>22.62</td>\n",
       "      <td>411.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>448.0</td>\n",
       "      <td>23.49</td>\n",
       "      <td>501.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Central</td>\n",
       "      <td>122062517</td>\n",
       "      <td>2968</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-06-03 13:00:00</td>\n",
       "      <td>2016-06-03 14:00:00</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>...</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>373.0</td>\n",
       "      <td>22.62</td>\n",
       "      <td>411.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>448.0</td>\n",
       "      <td>23.49</td>\n",
       "      <td>501.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Region  OwnerCode UnitCode  UnitType    BeginningTimeEST  \\\n",
       "0  Central  122062517     2968         4 2016-06-03 01:00:00   \n",
       "1  Central  122062517     2968         4 2016-06-03 03:00:00   \n",
       "2  Central  122062517     2968         4 2016-06-03 10:00:00   \n",
       "3  Central  122062517     2968         4 2016-06-03 12:00:00   \n",
       "4  Central  122062517     2968         4 2016-06-03 13:00:00   \n",
       "\n",
       "           EndTimeEST  EconomicMax  EconomicMin  EmergencyMax  EmergencyMin  \\\n",
       "0 2016-06-03 02:00:00        495.0        255.0         495.0         255.0   \n",
       "1 2016-06-03 04:00:00        495.0        255.0         495.0         255.0   \n",
       "2 2016-06-03 11:00:00        495.0        255.0         495.0         255.0   \n",
       "3 2016-06-03 13:00:00        495.0        255.0         495.0         255.0   \n",
       "4 2016-06-03 14:00:00        495.0        255.0         495.0         255.0   \n",
       "\n",
       "   ...    MW6  Price7    MW7  Price8    MW8  Price9    MW9  Price10   MW10  \\\n",
       "0  ...  343.0   22.25  373.0   22.62  411.0   22.98  448.0    23.49  501.0   \n",
       "1  ...  343.0   22.25  373.0   22.62  411.0   22.98  448.0    23.49  501.0   \n",
       "2  ...  343.0   22.25  373.0   22.62  411.0   22.98  448.0    23.49  501.0   \n",
       "3  ...  343.0   22.25  373.0   22.62  411.0   22.98  448.0    23.49  501.0   \n",
       "4  ...  343.0   22.25  373.0   22.62  411.0   22.98  448.0    23.49  501.0   \n",
       "\n",
       "   Slope  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miso_offers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970e561",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "\n",
    "To clean the data, I will begin by renaming the columns to improve their consistency and clarity. Following this, I will remove several columns such as CurtailmentOfferPrice and TargetMWReduction, as they contain only NA values. I will also remove UnitType, OwnerCode, Economic Flag, Unit Available flag, Slope, and Emergency Flag, as these are no longer relevant according to the Miso Offers report. Next, I will remove any rows containing NA values and set the Beginning time as the index for time series modeling. Finally, I will eliminate any outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "14273a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>UnitCode</th>\n",
       "      <th>BeginningTimeEST</th>\n",
       "      <th>EndTimeEST</th>\n",
       "      <th>EconomicMax</th>\n",
       "      <th>EconomicMin</th>\n",
       "      <th>EmergencyMax</th>\n",
       "      <th>EmergencyMin</th>\n",
       "      <th>MustRunFlag</th>\n",
       "      <th>SelfScheduledMW</th>\n",
       "      <th>...</th>\n",
       "      <th>Price6Offers</th>\n",
       "      <th>MW6Offers</th>\n",
       "      <th>Price7Offers</th>\n",
       "      <th>MW7Offers</th>\n",
       "      <th>Price8Offers</th>\n",
       "      <th>MW8Offers</th>\n",
       "      <th>Price9Offers</th>\n",
       "      <th>MW9Offers</th>\n",
       "      <th>Price10Offers</th>\n",
       "      <th>MW10Offers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2968</td>\n",
       "      <td>2016-06-03 01:00:00</td>\n",
       "      <td>2016-06-03 02:00:00</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.96</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>373.0</td>\n",
       "      <td>22.62</td>\n",
       "      <td>411.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>448.0</td>\n",
       "      <td>23.49</td>\n",
       "      <td>501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2968</td>\n",
       "      <td>2016-06-03 03:00:00</td>\n",
       "      <td>2016-06-03 04:00:00</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.96</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>373.0</td>\n",
       "      <td>22.62</td>\n",
       "      <td>411.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>448.0</td>\n",
       "      <td>23.49</td>\n",
       "      <td>501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2968</td>\n",
       "      <td>2016-06-03 10:00:00</td>\n",
       "      <td>2016-06-03 11:00:00</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.96</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>373.0</td>\n",
       "      <td>22.62</td>\n",
       "      <td>411.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>448.0</td>\n",
       "      <td>23.49</td>\n",
       "      <td>501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2968</td>\n",
       "      <td>2016-06-03 12:00:00</td>\n",
       "      <td>2016-06-03 13:00:00</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.96</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>373.0</td>\n",
       "      <td>22.62</td>\n",
       "      <td>411.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>448.0</td>\n",
       "      <td>23.49</td>\n",
       "      <td>501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2968</td>\n",
       "      <td>2016-06-03 13:00:00</td>\n",
       "      <td>2016-06-03 14:00:00</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.96</td>\n",
       "      <td>343.0</td>\n",
       "      <td>22.25</td>\n",
       "      <td>373.0</td>\n",
       "      <td>22.62</td>\n",
       "      <td>411.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>448.0</td>\n",
       "      <td>23.49</td>\n",
       "      <td>501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554995</th>\n",
       "      <td>1</td>\n",
       "      <td>6118</td>\n",
       "      <td>2016-04-12 16:00:00</td>\n",
       "      <td>2016-04-12 17:00:00</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554996</th>\n",
       "      <td>1</td>\n",
       "      <td>6118</td>\n",
       "      <td>2016-04-12 19:00:00</td>\n",
       "      <td>2016-04-12 20:00:00</td>\n",
       "      <td>47.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554997</th>\n",
       "      <td>1</td>\n",
       "      <td>6118</td>\n",
       "      <td>2016-04-12 20:00:00</td>\n",
       "      <td>2016-04-12 21:00:00</td>\n",
       "      <td>61.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554998</th>\n",
       "      <td>1</td>\n",
       "      <td>6118</td>\n",
       "      <td>2016-04-12 21:00:00</td>\n",
       "      <td>2016-04-12 22:00:00</td>\n",
       "      <td>70.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554999</th>\n",
       "      <td>1</td>\n",
       "      <td>6118</td>\n",
       "      <td>2016-04-12 22:00:00</td>\n",
       "      <td>2016-04-12 23:00:00</td>\n",
       "      <td>73.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1555000 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Region UnitCode    BeginningTimeEST          EndTimeEST  EconomicMax  \\\n",
       "0             1     2968 2016-06-03 01:00:00 2016-06-03 02:00:00        495.0   \n",
       "1             1     2968 2016-06-03 03:00:00 2016-06-03 04:00:00        495.0   \n",
       "2             1     2968 2016-06-03 10:00:00 2016-06-03 11:00:00        495.0   \n",
       "3             1     2968 2016-06-03 12:00:00 2016-06-03 13:00:00        495.0   \n",
       "4             1     2968 2016-06-03 13:00:00 2016-06-03 14:00:00        495.0   \n",
       "...         ...      ...                 ...                 ...          ...   \n",
       "1554995       1     6118 2016-04-12 16:00:00 2016-04-12 17:00:00         12.6   \n",
       "1554996       1     6118 2016-04-12 19:00:00 2016-04-12 20:00:00         47.2   \n",
       "1554997       1     6118 2016-04-12 20:00:00 2016-04-12 21:00:00         61.6   \n",
       "1554998       1     6118 2016-04-12 21:00:00 2016-04-12 22:00:00         70.1   \n",
       "1554999       1     6118 2016-04-12 22:00:00 2016-04-12 23:00:00         73.3   \n",
       "\n",
       "         EconomicMin  EmergencyMax  EmergencyMin  MustRunFlag  \\\n",
       "0              255.0         495.0         255.0            1   \n",
       "1              255.0         495.0         255.0            1   \n",
       "2              255.0         495.0         255.0            1   \n",
       "3              255.0         495.0         255.0            1   \n",
       "4              255.0         495.0         255.0            1   \n",
       "...              ...           ...           ...          ...   \n",
       "1554995          0.0          12.6           0.0            1   \n",
       "1554996          0.0          47.2           0.0            1   \n",
       "1554997          0.0          61.6           0.0            1   \n",
       "1554998          0.0          70.1           0.0            1   \n",
       "1554999          0.0          73.3           0.0            1   \n",
       "\n",
       "         SelfScheduledMW  ...  Price6Offers  MW6Offers  Price7Offers  \\\n",
       "0                    0.0  ...         21.96      343.0         22.25   \n",
       "1                    0.0  ...         21.96      343.0         22.25   \n",
       "2                    0.0  ...         21.96      343.0         22.25   \n",
       "3                    0.0  ...         21.96      343.0         22.25   \n",
       "4                    0.0  ...         21.96      343.0         22.25   \n",
       "...                  ...  ...           ...        ...           ...   \n",
       "1554995              0.0  ...           NaN        NaN           NaN   \n",
       "1554996              0.0  ...           NaN        NaN           NaN   \n",
       "1554997              0.0  ...           NaN        NaN           NaN   \n",
       "1554998              0.0  ...           NaN        NaN           NaN   \n",
       "1554999              0.0  ...           NaN        NaN           NaN   \n",
       "\n",
       "         MW7Offers  Price8Offers  MW8Offers  Price9Offers  MW9Offers  \\\n",
       "0            373.0         22.62      411.0         22.98      448.0   \n",
       "1            373.0         22.62      411.0         22.98      448.0   \n",
       "2            373.0         22.62      411.0         22.98      448.0   \n",
       "3            373.0         22.62      411.0         22.98      448.0   \n",
       "4            373.0         22.62      411.0         22.98      448.0   \n",
       "...            ...           ...        ...           ...        ...   \n",
       "1554995        NaN           NaN        NaN           NaN        NaN   \n",
       "1554996        NaN           NaN        NaN           NaN        NaN   \n",
       "1554997        NaN           NaN        NaN           NaN        NaN   \n",
       "1554998        NaN           NaN        NaN           NaN        NaN   \n",
       "1554999        NaN           NaN        NaN           NaN        NaN   \n",
       "\n",
       "         Price10Offers  MW10Offers  \n",
       "0                23.49       501.0  \n",
       "1                23.49       501.0  \n",
       "2                23.49       501.0  \n",
       "3                23.49       501.0  \n",
       "4                23.49       501.0  \n",
       "...                ...         ...  \n",
       "1554995            NaN         NaN  \n",
       "1554996            NaN         NaN  \n",
       "1554997            NaN         NaN  \n",
       "1554998            NaN         NaN  \n",
       "1554999            NaN         NaN  \n",
       "\n",
       "[1555000 rows x 32 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miso_offers_df.rename(columns={'Price1': 'Price1Offers', 'MW1':'MW1Offers', \n",
    "                               'Price2': 'Price2Offers', 'MW2':'MW2Offers', \n",
    "                               'Price3': 'Price3Offers', 'MW3':'MW3Offers',  \n",
    "                               'Price4': 'Price4Offers', 'MW4':'MW4Offers', \n",
    "                               'Price5': 'Price5Offers', 'MW5':'MW5Offers', \n",
    "                               'Price6': 'Price6Offers', 'MW6':'MW6Offers', \n",
    "                               'Price7': 'Price7Offers', 'MW7':'MW7Offers', \n",
    "                               'Price8': 'Price8Offers', 'MW8':'MW8Offers', \n",
    "                               'Price9': 'Price9Offers', 'MW9':'MW9Offers',\n",
    "                               'Price10': 'Price10Offers', 'MW10':'MW10Offers',\n",
    "                               'LMP': 'LMPOffers', 'MW': 'MWOffers'}, inplace=True)\n",
    "miso_offers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6d63a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c2141b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove CurtailmentOfferPrice, TargetMWReduction\n",
    "# remove UnitType, OwnerCode, Economic Flag, Unit Available flag, Slope,\n",
    "# and Emergency Flag \n",
    "miso_offers_df = miso_offers_df.drop(columns=['CurtailmentOfferPrice', 'TargetMWReduction', \n",
    "                                              'UnitType', 'OwnerCode', 'EconomicFlag', \n",
    "                                              'UnitAvailableFlag', 'EmergencyFlag', 'Slope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6473d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NAs\n",
    "miso_offers_df = miso_offers_df.dropna(subset=['EconomicMax', 'EconomicMin', \n",
    "                                               'EmergencyMax', 'EmergencyMin',\n",
    "                                               'Price1Offers', 'MW1Offers', 'Price2Offers', 'MW2Offers', \n",
    "                                               'Price3Offers', 'MW3Offers', 'Price4Offers', 'MW4Offers', \n",
    "                                               'Price5Offers', 'MW5Offers', 'Price6Offers', 'MW6Offers',\n",
    "                                               'Price7Offers', 'MW7Offers', 'Price8Offers', 'MW8Offers', \n",
    "                                               'Price9Offers', 'MW9Offers', 'Price10Offers', 'MW10Offers', \n",
    "                                               'LMPOffers', 'MWOffers', 'SelfScheduledMW',\n",
    "                                               'MustRunFlag', 'UnitCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b9089cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "miso_offers_df = miso_offers_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dcdae7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Region to Categorical\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the Region column to categorical\n",
    "miso_offers_df['Region'] = pd.Categorical(miso_offers_df['Region'], \n",
    "                                          categories=['Central', 'South', 'North'], ordered=True)\n",
    "miso_offers_df['Region'] = miso_offers_df['Region'].cat.codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9fcc005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 89747 entries, 0 to 1554991\n",
      "Data columns (total 32 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   Region            89747 non-null  int8          \n",
      " 1   UnitCode          89747 non-null  object        \n",
      " 2   BeginningTimeEST  89747 non-null  datetime64[ns]\n",
      " 3   EndTimeEST        89747 non-null  datetime64[ns]\n",
      " 4   EconomicMax       89747 non-null  float64       \n",
      " 5   EconomicMin       89747 non-null  float64       \n",
      " 6   EmergencyMax      89747 non-null  float64       \n",
      " 7   EmergencyMin      89747 non-null  float64       \n",
      " 8   MustRunFlag       89747 non-null  Int64         \n",
      " 9   SelfScheduledMW   89747 non-null  float64       \n",
      " 10  MWOffers          89747 non-null  float64       \n",
      " 11  LMPOffers         89747 non-null  float64       \n",
      " 12  Price1Offers      89747 non-null  float64       \n",
      " 13  MW1Offers         89747 non-null  float64       \n",
      " 14  Price2Offers      89747 non-null  float64       \n",
      " 15  MW2Offers         89747 non-null  float64       \n",
      " 16  Price3Offers      89747 non-null  float64       \n",
      " 17  MW3Offers         89747 non-null  float64       \n",
      " 18  Price4Offers      89747 non-null  float64       \n",
      " 19  MW4Offers         89747 non-null  float64       \n",
      " 20  Price5Offers      89747 non-null  float64       \n",
      " 21  MW5Offers         89747 non-null  float64       \n",
      " 22  Price6Offers      89747 non-null  float64       \n",
      " 23  MW6Offers         89747 non-null  float64       \n",
      " 24  Price7Offers      89747 non-null  float64       \n",
      " 25  MW7Offers         89747 non-null  float64       \n",
      " 26  Price8Offers      89747 non-null  float64       \n",
      " 27  MW8Offers         89747 non-null  float64       \n",
      " 28  Price9Offers      89747 non-null  float64       \n",
      " 29  MW9Offers         89747 non-null  float64       \n",
      " 30  Price10Offers     89747 non-null  float64       \n",
      " 31  MW10Offers        89747 non-null  float64       \n",
      "dtypes: Int64(1), datetime64[ns](2), float64(27), int8(1), object(1)\n",
      "memory usage: 22.1+ MB\n"
     ]
    }
   ],
   "source": [
    "miso_offers_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"BeginningTimeEST\" column to a datetime type\n",
    "df[\"BeginningTimeEST\"] = pd.to_datetime(df[\"BeginningTimeEST\"])\n",
    "\n",
    "# Get the minimum and maximum date from the column\n",
    "min_date = df[\"BeginningTimeEST\"].min()\n",
    "max_date = df[\"BeginningTimeEST\"].max()\n",
    "\n",
    "# Print the date range\n",
    "print(\"Date range: {} to {}\".format(min_date.date(), max_date.date()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0b90fe",
   "metadata": {},
   "source": [
    "#### Set date-time as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d8ccd4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "miso_offers_df.set_index('BeginningTimeEST', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2dd24cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 89747 entries, 2016-06-03 01:00:00 to 2016-04-12 21:00:00\n",
      "Data columns (total 31 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   Region           89747 non-null  int8          \n",
      " 1   UnitCode         89747 non-null  object        \n",
      " 2   EndTimeEST       89747 non-null  datetime64[ns]\n",
      " 3   EconomicMax      89747 non-null  float64       \n",
      " 4   EconomicMin      89747 non-null  float64       \n",
      " 5   EmergencyMax     89747 non-null  float64       \n",
      " 6   EmergencyMin     89747 non-null  float64       \n",
      " 7   MustRunFlag      89747 non-null  Int64         \n",
      " 8   SelfScheduledMW  89747 non-null  float64       \n",
      " 9   MWOffers         89747 non-null  float64       \n",
      " 10  LMPOffers        89747 non-null  float64       \n",
      " 11  Price1Offers     89747 non-null  float64       \n",
      " 12  MW1Offers        89747 non-null  float64       \n",
      " 13  Price2Offers     89747 non-null  float64       \n",
      " 14  MW2Offers        89747 non-null  float64       \n",
      " 15  Price3Offers     89747 non-null  float64       \n",
      " 16  MW3Offers        89747 non-null  float64       \n",
      " 17  Price4Offers     89747 non-null  float64       \n",
      " 18  MW4Offers        89747 non-null  float64       \n",
      " 19  Price5Offers     89747 non-null  float64       \n",
      " 20  MW5Offers        89747 non-null  float64       \n",
      " 21  Price6Offers     89747 non-null  float64       \n",
      " 22  MW6Offers        89747 non-null  float64       \n",
      " 23  Price7Offers     89747 non-null  float64       \n",
      " 24  MW7Offers        89747 non-null  float64       \n",
      " 25  Price8Offers     89747 non-null  float64       \n",
      " 26  MW8Offers        89747 non-null  float64       \n",
      " 27  Price9Offers     89747 non-null  float64       \n",
      " 28  MW9Offers        89747 non-null  float64       \n",
      " 29  Price10Offers    89747 non-null  float64       \n",
      " 30  MW10Offers       89747 non-null  float64       \n",
      "dtypes: Int64(1), datetime64[ns](1), float64(27), int8(1), object(1)\n",
      "memory usage: 21.4+ MB\n"
     ]
    }
   ],
   "source": [
    "miso_offers_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825202f",
   "metadata": {},
   "source": [
    "#### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dcb62407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(89747, 30)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "miso_offers_if = miso_offers_df[['EconomicMax', 'EconomicMin', 'Region',\n",
    "                                               'EmergencyMax', 'EmergencyMin',\n",
    "                                               'Price1Offers', 'MW1Offers', 'Price2Offers', 'MW2Offers', \n",
    "                                               'Price3Offers', 'MW3Offers', 'Price4Offers', 'MW4Offers', \n",
    "                                               'Price5Offers', 'MW5Offers', 'Price6Offers', 'MW6Offers',\n",
    "                                               'Price7Offers', 'MW7Offers', 'Price8Offers', 'MW8Offers', \n",
    "                                               'Price9Offers', 'MW9Offers', 'Price10Offers', 'MW10Offers', \n",
    "                                               'LMPOffers', 'MWOffers', 'SelfScheduledMW',\n",
    "                                               'MustRunFlag', 'UnitCode']]\n",
    "\n",
    "#Train an Isolation Forest\n",
    "model = IsolationForest(contamination = 0.1)\n",
    "model.fit(miso_offers_if)\n",
    "\n",
    "#Predict anomalies\n",
    "predictions = model.predict(miso_offers_if)\n",
    "\n",
    "# Get the indices of the rows with positive value in predictions\n",
    "positive_indices = [i for i, x in enumerate(predictions) if x>0]\n",
    "\n",
    "# Subset the dataframe using the indices\n",
    "df_trimmed = miso_offers_if.iloc[positive_indices]\n",
    "\n",
    "miso_offers_if.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "665c90e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80773, 30)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trimmed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7d37f",
   "metadata": {},
   "source": [
    "A total of 8,974 rows were identified as outliers in the data through the use of Isolation Forests and were subsequently removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd5b03",
   "metadata": {},
   "source": [
    "#### Splitting Dataset\n",
    "\n",
    "In this section, I will divide the dataset into a training set and a test set using the Min-Max scaler. The training set will comprise 60% of the data while the test set will consist of the remaining 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5736b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "39e02105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_size = int(len(miso_offers) * 0.6)\n",
    "train, test = miso_offers.iloc[:train_size], miso_offers.iloc[train_size:]\n",
    "\n",
    "# Scale the data as a numpy array\n",
    "scaler = MinMaxScaler()\n",
    "train_sc = scaler.fit_transform(train)\n",
    "test_sc = scaler.transform(test)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame with datetime index\n",
    "train_sc_df = pd.DataFrame(train_sc, columns=train.columns, index=train.index)\n",
    "test_sc_df = pd.DataFrame(test_sc, columns=test.columns, index=test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a4480771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                      EconomicMax  EconomicMin  Region  EmergencyMax  \\\n",
       "BeginningTimeEST                                                      \n",
       "2016-06-03 01:00:00     0.793269     0.415987     0.0      0.760369   \n",
       "2016-06-03 03:00:00     0.793269     0.415987     0.0      0.760369   \n",
       "2016-06-03 10:00:00     0.793269     0.415987     0.0      0.760369   \n",
       "2016-06-03 12:00:00     0.793269     0.415987     0.0      0.760369   \n",
       "2016-06-03 13:00:00     0.793269     0.415987     0.0      0.760369   \n",
       "...                          ...          ...     ...           ...   \n",
       "2016-12-26 19:00:00     0.145833     0.089723     0.0      0.152074   \n",
       "2016-12-26 02:00:00     0.145833     0.089723     0.0      0.152074   \n",
       "2016-12-26 03:00:00     0.145833     0.089723     0.0      0.152074   \n",
       "2016-12-26 07:00:00     0.145833     0.089723     0.0      0.152074   \n",
       "2016-12-26 08:00:00     0.145833     0.089723     0.0      0.152074   \n",
       "\n",
       "                     EmergencyMin  Price1Offers  MW1Offers  Price2Offers  \\\n",
       "BeginningTimeEST                                                           \n",
       "2016-06-03 01:00:00      0.493230      0.201343   0.588235      0.146100   \n",
       "2016-06-03 03:00:00      0.493230      0.201343   0.588235      0.146100   \n",
       "2016-06-03 10:00:00      0.493230      0.201343   0.588235      0.146100   \n",
       "2016-06-03 12:00:00      0.493230      0.201343   0.588235      0.146100   \n",
       "2016-06-03 13:00:00      0.493230      0.201343   0.588235      0.146100   \n",
       "...                           ...           ...        ...           ...   \n",
       "2016-12-26 19:00:00      0.106383      0.340927   0.161765      0.296135   \n",
       "2016-12-26 02:00:00      0.106383      0.347511   0.161765      0.303162   \n",
       "2016-12-26 03:00:00      0.106383      0.347511   0.161765      0.303162   \n",
       "2016-12-26 07:00:00      0.106383      0.347511   0.161765      0.303162   \n",
       "2016-12-26 08:00:00      0.106383      0.347511   0.161765      0.303162   \n",
       "\n",
       "                     MW2Offers  Price3Offers  ...  MW8Offers  Price9Offers  \\\n",
       "BeginningTimeEST                              ...                            \n",
       "2016-06-03 01:00:00   0.488789      0.147772  ...   0.676148      0.152387   \n",
       "2016-06-03 03:00:00   0.488789      0.147772  ...   0.676148      0.152387   \n",
       "2016-06-03 10:00:00   0.488789      0.147772  ...   0.676148      0.152387   \n",
       "2016-06-03 12:00:00   0.488789      0.147772  ...   0.676148      0.152387   \n",
       "2016-06-03 13:00:00   0.488789      0.147772  ...   0.676148      0.152387   \n",
       "...                        ...           ...  ...        ...           ...   \n",
       "2016-12-26 19:00:00   0.123318      0.297786  ...   0.114119      0.296220   \n",
       "2016-12-26 02:00:00   0.123318      0.304793  ...   0.114119      0.302851   \n",
       "2016-12-26 03:00:00   0.123318      0.304793  ...   0.114119      0.302851   \n",
       "2016-12-26 07:00:00   0.123318      0.304793  ...   0.114119      0.302851   \n",
       "2016-12-26 08:00:00   0.123318      0.304793  ...   0.114119      0.302851   \n",
       "\n",
       "                     MW9Offers  Price10Offers  MW10Offers  LMPOffers  \\\n",
       "BeginningTimeEST                                                       \n",
       "2016-06-03 01:00:00   0.719720       0.153389    0.763482   0.427279   \n",
       "2016-06-03 03:00:00   0.719720       0.153389    0.763482   0.419136   \n",
       "2016-06-03 10:00:00   0.719720       0.153389    0.763482   0.521677   \n",
       "2016-06-03 12:00:00   0.719720       0.153389    0.763482   0.539320   \n",
       "2016-06-03 13:00:00   0.719720       0.153389    0.763482   0.555153   \n",
       "...                        ...            ...         ...        ...   \n",
       "2016-12-26 19:00:00   0.117451       0.294045    0.116998   0.452386   \n",
       "2016-12-26 02:00:00   0.117451       0.300575    0.116998   0.352560   \n",
       "2016-12-26 03:00:00   0.117451       0.300575    0.116998   0.348564   \n",
       "2016-12-26 07:00:00   0.117451       0.300575    0.116998   0.373973   \n",
       "2016-12-26 08:00:00   0.117451       0.300575    0.116998   0.399457   \n",
       "\n",
       "                     MWOffers  SelfScheduledMW  MustRunFlag  UnitCode  \n",
       "BeginningTimeEST                                                       \n",
       "2016-06-03 01:00:00  0.408654              0.0          1.0  0.000000  \n",
       "2016-06-03 03:00:00  0.408654              0.0          1.0  0.000000  \n",
       "2016-06-03 10:00:00  0.793269              0.0          1.0  0.000000  \n",
       "2016-06-03 12:00:00  0.793269              0.0          1.0  0.000000  \n",
       "2016-06-03 13:00:00  0.793269              0.0          1.0  0.000000  \n",
       "...                       ...              ...          ...       ...  \n",
       "2016-12-26 19:00:00  0.000000              0.0          0.0  0.079507  \n",
       "2016-12-26 02:00:00  0.000000              0.0          0.0  0.079683  \n",
       "2016-12-26 03:00:00  0.000000              0.0          0.0  0.079683  \n",
       "2016-12-26 07:00:00  0.000000              0.0          0.0  0.079683  \n",
       "2016-12-26 08:00:00  0.000000              0.0          0.0  0.079683  \n",
       "\n",
       "[48463 rows x 30 columns]>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sc_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8b39f245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48463, 29) (48463,)\n"
     ]
    }
   ],
   "source": [
    "# Here I am splitting the scaled dataset into X and Y dataframes\n",
    "X_train = train_sc_df[['Region',\n",
    "                                'EconomicMax','EconomicMin','EmergencyMax',\n",
    "                                'EmergencyMin','MustRunFlag','SelfScheduledMW','MWOffers',\n",
    "                                'Price1Offers','MW1Offers','Price2Offers','MW2Offers','Price3Offers','MW3Offers','Price4Offers',\n",
    "                                'MW4Offers','Price5Offers','MW5Offers','Price6Offers','MW6Offers','Price7Offers','MW7Offers',\n",
    "                                'Price8Offers','MW8Offers','Price9Offers','MW9Offers','Price10Offers','MW10Offers', 'UnitCode']]\n",
    "y_train = train_sc_df['LMPOffers']\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "91f065a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32310, 29) (32310,)\n"
     ]
    }
   ],
   "source": [
    "# Here I am splitting the scaled dataset into X and Y dataframes\n",
    "X_test = test_sc_df[['Region',\n",
    "                                'EconomicMax','EconomicMin','EmergencyMax',\n",
    "                                'EmergencyMin','MustRunFlag','SelfScheduledMW','MWOffers',\n",
    "                                'Price1Offers','MW1Offers','Price2Offers','MW2Offers','Price3Offers','MW3Offers','Price4Offers',\n",
    "                                'MW4Offers','Price5Offers','MW5Offers','Price6Offers','MW6Offers','Price7Offers','MW7Offers',\n",
    "                                'Price8Offers','MW8Offers','Price9Offers','MW9Offers','Price10Offers','MW10Offers', 'UnitCode']]\n",
    "y_test = test_sc_df['LMPOffers']\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd490b8",
   "metadata": {},
   "source": [
    "The training set has of 48,463 rows and the test set has of 32,310. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422fc21b",
   "metadata": {},
   "source": [
    "#### Models\n",
    "\n",
    "##### Simple Neural Network Model\n",
    "\n",
    "In this section, I will enhance the previous neural network model by incorporating more data and increasing the number of epochs. The previous model was trained and tested on 4,327 and 2,886 rows respectively and achieved a training accuracy of 0.337 and testing accuracy of 0.292. The goal of this model is to improve upon those results.\n",
    "\n",
    "This model uses the ReLU activation that is applied to the output of each neuron in a neural network. It is a simple and efficient function that allows the network to learn nonlinear relationships between the input and output. The model also uses the Adam optimizer that is used to update the weights of the neural network during the training process. It is a stochastic gradient descent method which computes individual adaptive learning rates for different parameters, making it an efficient algorithm for large datasets and complex models. Using the Adam optimizer can help speed up the training process and improve the overall performance of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d3fd88db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0072 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0043 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 11/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1767/1767 [==============================] - 6s 4ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1767/1767 [==============================] - 10s 6ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 24/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 28/200\n",
      "1767/1767 [==============================] - 4s 2ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 36/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1767/1767 [==============================] - 6s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 43/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1767/1767 [==============================] - 6s 4ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 47/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 51/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0034 - accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 63/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 73/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1767/1767 [==============================] - 4s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1767/1767 [==============================] - 4s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1767/1767 [==============================] - 6s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "1767/1767 [==============================] - 6s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "1767/1767 [==============================] - 6s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 93/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 96/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 101/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0033 - accuracy: 0.0000e+00\n",
      "Epoch 102/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 105/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 106/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 108/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "1767/1767 [==============================] - 4s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "1767/1767 [==============================] - 4s 2ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 115/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "1767/1767 [==============================] - 4s 2ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "1767/1767 [==============================] - 4s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "1767/1767 [==============================] - 4s 2ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "1767/1767 [==============================] - 4s 2ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "1767/1767 [==============================] - 4s 2ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 128/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "1767/1767 [==============================] - 4s 2ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "1767/1767 [==============================] - 10s 6ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "1767/1767 [==============================] - 8s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 141/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 144/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 147/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 159/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 161/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 168/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 172/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "1767/1767 [==============================] - 4s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "1767/1767 [==============================] - 4s 2ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 181/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 184/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "1767/1767 [==============================] - 6s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 186/200\n",
      "1767/1767 [==============================] - 6s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 191/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1767/1767 [==============================] - 6s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1767/1767 [==============================] - 6s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1767/1767 [==============================] - 9s 5ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1767/1767 [==============================] - 6s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1767/1767 [==============================] - 5s 3ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1767/1767 [==============================] - 7s 4ms/step - loss: 0.0032 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def neural_network(X, y):\n",
    "    # Define the model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(50, activation='relu', input_shape=[28]))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model to the data\n",
    "    history = model.fit(X, y, epochs=200)\n",
    "\n",
    "    return model\n",
    "\n",
    "basic_model = neural_network(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a203f62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 5s 3ms/step\n",
      "1010/1010 [==============================] - 2s 2ms/step\n",
      "Simple Neural Network Model Train accuracy: 0.44120630450457465\n",
      "Simple Neural Network Model Test accuracy: 0.4384352074929001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Make predictions\n",
    "train_preds = basic_model.predict(X_train)\n",
    "test_preds = basic_model.predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, train_preds)\n",
    "test_r2 = r2_score(y_test, test_preds)\n",
    "\n",
    "print(f\"Simple Neural Network Model Train accuracy: {train_r2}\")\n",
    "print(f\"Simple Neural Network Model Test accuracy: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea82a5",
   "metadata": {},
   "source": [
    "The simple neural network model in this section was able to achieve a training accuracy of 0.441 and testing accuracy of 0.438. This is a notable improvement compared to the previous notebook's model, which had a training accuracy of 0.337 and testing accuracy of 0.292. The larger amount of data used in this model may have contributed to this improvement, with the training set consisting of 48,463 rows and the test set consisting of 32,310 rows, more than 10 times the amount of data in the previous notebook. This model was trained for 200 epochs compared to the previous model's 150 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58fcc6",
   "metadata": {},
   "source": [
    "##### RNN\n",
    "\n",
    "This section applies a Recurrent Neural Network to the data. A Recurrent Neural Network works with sequential data, such as time-series data like in this notebook. Unlike traditional feedforward neural networks, RNNs have loops that allow information to persist from one step of the sequence to the next. This means that RNNs are able to capture temporal dependencies and make predictions based on past events.\n",
    "\n",
    "An RNN processes input sequences one element at a time and maintains a hidden state that represents the network's \"memory\" of the previous inputs. At each step, the input is combined with the previous hidden state to produce a new hidden state and an output. This output is then fed back into the network as input for the next step, allowing the network to make predictions that depend on the entire input sequence.\n",
    "\n",
    "This RNN model includes 1 input SimpleRNN layer, 2 hidden SimpleRNN layers and 1 output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ed8cfc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "\n",
    "def rnn(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.SimpleRNN(units=64, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(layers.SimpleRNN(units=32, return_sequences=True))\n",
    "    model.add(layers.SimpleRNN(units=16))\n",
    "    model.add(layers.Dense(units=1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model to the data\n",
    "    history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d4d5d1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1767/1767 [==============================] - 62s 32ms/step - loss: 0.0068 - accuracy: 0.0000e+00 - val_loss: 0.0052 - val_accuracy: 3.0950e-05\n",
      "Epoch 2/200\n",
      "1767/1767 [==============================] - 57s 32ms/step - loss: 0.0047 - accuracy: 0.0000e+00 - val_loss: 0.0047 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "1767/1767 [==============================] - 55s 31ms/step - loss: 0.0046 - accuracy: 0.0000e+00 - val_loss: 0.0046 - val_accuracy: 3.0950e-05\n",
      "Epoch 4/200\n",
      "1767/1767 [==============================] - 56s 32ms/step - loss: 0.0045 - accuracy: 0.0000e+00 - val_loss: 0.0048 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "1767/1767 [==============================] - 62s 35ms/step - loss: 0.0044 - accuracy: 0.0000e+00 - val_loss: 0.0053 - val_accuracy: 3.0950e-05\n",
      "Epoch 6/200\n",
      "1767/1767 [==============================] - 63s 36ms/step - loss: 0.0043 - accuracy: 0.0000e+00 - val_loss: 0.0043 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/200\n",
      "1767/1767 [==============================] - 62s 35ms/step - loss: 0.0043 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/200\n",
      "1767/1767 [==============================] - 72s 41ms/step - loss: 0.0042 - accuracy: 0.0000e+00 - val_loss: 0.0043 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/200\n",
      "1767/1767 [==============================] - 68s 39ms/step - loss: 0.0042 - accuracy: 0.0000e+00 - val_loss: 0.0045 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/200\n",
      "1767/1767 [==============================] - 54s 30ms/step - loss: 0.0041 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 3.0950e-05\n",
      "Epoch 11/200\n",
      "1767/1767 [==============================] - 58s 33ms/step - loss: 0.0041 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/200\n",
      "1767/1767 [==============================] - 61s 34ms/step - loss: 0.0040 - accuracy: 0.0000e+00 - val_loss: 0.0042 - val_accuracy: 3.0950e-05\n",
      "Epoch 13/200\n",
      "1767/1767 [==============================] - 53s 30ms/step - loss: 0.0040 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "1767/1767 [==============================] - 52s 30ms/step - loss: 0.0040 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/200\n",
      "1767/1767 [==============================] - 49s 27ms/step - loss: 0.0039 - accuracy: 0.0000e+00 - val_loss: 0.0041 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0039 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "1767/1767 [==============================] - 53s 30ms/step - loss: 0.0039 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "1767/1767 [==============================] - 59s 34ms/step - loss: 0.0039 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/200\n",
      "1767/1767 [==============================] - 52s 30ms/step - loss: 0.0038 - accuracy: 0.0000e+00 - val_loss: 0.0042 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/200\n",
      "1767/1767 [==============================] - 54s 31ms/step - loss: 0.0038 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "1767/1767 [==============================] - 60s 34ms/step - loss: 0.0038 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "1767/1767 [==============================] - 63s 36ms/step - loss: 0.0038 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "1767/1767 [==============================] - 53s 30ms/step - loss: 0.0038 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 3.0950e-05\n",
      "Epoch 24/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0038 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/200\n",
      "1767/1767 [==============================] - 54s 31ms/step - loss: 0.0038 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0038 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 3.0950e-05\n",
      "Epoch 28/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/200\n",
      "1767/1767 [==============================] - 49s 27ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/200\n",
      "1767/1767 [==============================] - 49s 27ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/200\n",
      "1767/1767 [==============================] - 52s 29ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/200\n",
      "1767/1767 [==============================] - 51s 29ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 3.0950e-05\n",
      "Epoch 36/200\n",
      "1767/1767 [==============================] - 49s 28ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/200\n",
      "1767/1767 [==============================] - 49s 28ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/200\n",
      "1767/1767 [==============================] - 51s 29ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/200\n",
      "1767/1767 [==============================] - 76s 43ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/200\n",
      "1767/1767 [==============================] - 94s 53ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/200\n",
      "1767/1767 [==============================] - 66s 37ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/200\n",
      "1767/1767 [==============================] - 58s 33ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 3.0950e-05\n",
      "Epoch 43/200\n",
      "1767/1767 [==============================] - 64s 36ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/200\n",
      "1767/1767 [==============================] - 80s 45ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/200\n",
      "1767/1767 [==============================] - 80s 45ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/200\n",
      "1767/1767 [==============================] - 83s 47ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 3.0950e-05\n",
      "Epoch 47/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "1767/1767 [==============================] - 46s 26ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/200\n",
      "1767/1767 [==============================] - 45s 26ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/200\n",
      "1767/1767 [==============================] - 44s 25ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 3.0950e-05\n",
      "Epoch 51/200\n",
      "1767/1767 [==============================] - 45s 25ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/200\n",
      "1767/1767 [==============================] - 47s 26ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/200\n",
      "1767/1767 [==============================] - 56s 32ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/200\n",
      "1767/1767 [==============================] - 49s 28ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/200\n",
      "1767/1767 [==============================] - 54s 31ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/200\n",
      "1767/1767 [==============================] - 45s 25ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/200\n",
      "1767/1767 [==============================] - 44s 25ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/200\n",
      "1767/1767 [==============================] - 45s 25ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/200\n",
      "1767/1767 [==============================] - 47s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/200\n",
      "1767/1767 [==============================] - 49s 28ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 3.0950e-05\n",
      "Epoch 63/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/200\n",
      "1767/1767 [==============================] - 49s 28ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/200\n",
      "1767/1767 [==============================] - 49s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/200\n",
      "1767/1767 [==============================] - 47s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 3.0950e-05\n",
      "Epoch 71/200\n",
      "1767/1767 [==============================] - 48s 27ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/200\n",
      "1767/1767 [==============================] - 54s 30ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 3.0950e-05\n",
      "Epoch 73/200\n",
      "1767/1767 [==============================] - 1796s 1s/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "1767/1767 [==============================] - 62s 35ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/200\n",
      "1767/1767 [==============================] - 61s 35ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/200\n",
      "1767/1767 [==============================] - 67s 38ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/200\n",
      "1767/1767 [==============================] - 64s 36ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/200\n",
      "1767/1767 [==============================] - 68s 38ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/200\n",
      "1767/1767 [==============================] - 63s 36ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/200\n",
      "1767/1767 [==============================] - 64s 37ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/200\n",
      "1767/1767 [==============================] - 65s 37ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/200\n",
      "1767/1767 [==============================] - 65s 37ms/step - loss: 0.0037 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/200\n",
      "1767/1767 [==============================] - 66s 38ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/200\n",
      "1767/1767 [==============================] - 67s 38ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/200\n",
      "1767/1767 [==============================] - 65s 37ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/200\n",
      "1767/1767 [==============================] - 68s 38ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/200\n",
      "1767/1767 [==============================] - 66s 37ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/200\n",
      "1767/1767 [==============================] - 67s 38ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/200\n",
      "1767/1767 [==============================] - 61s 35ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/200\n",
      "1767/1767 [==============================] - 66s 38ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/200\n",
      "1767/1767 [==============================] - 69s 39ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/200\n",
      "1767/1767 [==============================] - 67s 38ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 3.0950e-05\n",
      "Epoch 93/200\n",
      "1767/1767 [==============================] - 68s 39ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/200\n",
      "1767/1767 [==============================] - 80s 45ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/200\n",
      "1767/1767 [==============================] - 70s 40ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 3.0950e-05\n",
      "Epoch 96/200\n",
      "1767/1767 [==============================] - 73s 41ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/200\n",
      "1767/1767 [==============================] - 72s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/200\n",
      "1767/1767 [==============================] - 71s 40ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/200\n",
      "1767/1767 [==============================] - 80s 45ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/200\n",
      "1767/1767 [==============================] - 76s 43ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 3.0950e-05\n",
      "Epoch 101/200\n",
      "1767/1767 [==============================] - 84s 48ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 3.0950e-05\n",
      "Epoch 102/200\n",
      "1767/1767 [==============================] - 76s 43ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/200\n",
      "1767/1767 [==============================] - 73s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/200\n",
      "1767/1767 [==============================] - 75s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 3.0950e-05\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 74s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 3.0950e-05\n",
      "Epoch 106/200\n",
      "1767/1767 [==============================] - 72s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/200\n",
      "1767/1767 [==============================] - 74s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 3.0950e-05\n",
      "Epoch 108/200\n",
      "1767/1767 [==============================] - 72s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/200\n",
      "1767/1767 [==============================] - 72s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/200\n",
      "1767/1767 [==============================] - 75s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/200\n",
      "1767/1767 [==============================] - 73s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/200\n",
      "1767/1767 [==============================] - 73s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/200\n",
      "1767/1767 [==============================] - 73s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/200\n",
      "1767/1767 [==============================] - 74s 42ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 3.0950e-05\n",
      "Epoch 115/200\n",
      "1767/1767 [==============================] - 80s 45ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/200\n",
      "1767/1767 [==============================] - 74s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/200\n",
      "1767/1767 [==============================] - 86s 49ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/200\n",
      "1767/1767 [==============================] - 73s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/200\n",
      "1767/1767 [==============================] - 74s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/200\n",
      "1767/1767 [==============================] - 80s 45ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/200\n",
      "1767/1767 [==============================] - 75s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/200\n",
      "1767/1767 [==============================] - 77s 44ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/200\n",
      "1767/1767 [==============================] - 80s 45ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/200\n",
      "1767/1767 [==============================] - 79s 45ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/200\n",
      "1767/1767 [==============================] - 73s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/200\n",
      "1767/1767 [==============================] - 73s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/200\n",
      "1767/1767 [==============================] - 74s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 3.0950e-05\n",
      "Epoch 128/200\n",
      "1767/1767 [==============================] - 74s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/200\n",
      "1767/1767 [==============================] - 76s 43ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/200\n",
      "1767/1767 [==============================] - 76s 43ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/200\n",
      "1767/1767 [==============================] - 74s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/200\n",
      "1767/1767 [==============================] - 76s 43ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/200\n",
      "1767/1767 [==============================] - 85s 48ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/200\n",
      "1767/1767 [==============================] - 74s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/200\n",
      "1767/1767 [==============================] - 80s 45ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/200\n",
      "1767/1767 [==============================] - 76s 43ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/200\n",
      "1767/1767 [==============================] - 76s 43ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/200\n",
      "1767/1767 [==============================] - 56s 32ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/200\n",
      "1767/1767 [==============================] - 51s 29ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/200\n",
      "1767/1767 [==============================] - 46s 26ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 3.0950e-05\n",
      "Epoch 141/200\n",
      "1767/1767 [==============================] - 46s 26ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/200\n",
      "1767/1767 [==============================] - 45s 26ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/200\n",
      "1767/1767 [==============================] - 45s 26ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 3.0950e-05\n",
      "Epoch 144/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/200\n",
      "1767/1767 [==============================] - 49s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 3.0950e-05\n",
      "Epoch 147/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/200\n",
      "1767/1767 [==============================] - 47s 26ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/200\n",
      "1767/1767 [==============================] - 47s 27ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/200\n",
      "1767/1767 [==============================] - 46s 26ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 3.0950e-05\n",
      "Epoch 151/200\n",
      "1767/1767 [==============================] - 47s 27ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 152/200\n",
      "1767/1767 [==============================] - 49s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 153/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/200\n",
      "1767/1767 [==============================] - 49s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 155/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 157/200\n",
      "1767/1767 [==============================] - 49s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 158/200\n",
      "1767/1767 [==============================] - 50s 29ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 3.0950e-05\n",
      "Epoch 159/200\n",
      "1767/1767 [==============================] - 52s 30ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 160/200\n",
      "1767/1767 [==============================] - 52s 29ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0039 - val_accuracy: 3.0950e-05\n",
      "Epoch 161/200\n",
      "1767/1767 [==============================] - 51s 29ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 163/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 164/200\n",
      "1767/1767 [==============================] - 50s 28ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 165/200\n",
      "1767/1767 [==============================] - 49s 28ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 166/200\n",
      "1767/1767 [==============================] - 45s 26ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 167/200\n",
      "1767/1767 [==============================] - 1795s 1s/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 3.0950e-05\n",
      "Epoch 168/200\n",
      "1767/1767 [==============================] - 1012s 573ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 169/200\n",
      "1767/1767 [==============================] - 54s 30ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0040 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/200\n",
      "1767/1767 [==============================] - 65s 37ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/200\n",
      "1767/1767 [==============================] - 58s 33ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 3.0950e-05\n",
      "Epoch 172/200\n",
      "1767/1767 [==============================] - 59s 33ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 173/200\n",
      "1767/1767 [==============================] - 60s 34ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 174/200\n",
      "1767/1767 [==============================] - 61s 35ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/200\n",
      "1767/1767 [==============================] - 62s 35ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/200\n",
      "1767/1767 [==============================] - 62s 35ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 177/200\n",
      "1767/1767 [==============================] - 63s 36ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 178/200\n",
      "1767/1767 [==============================] - 63s 36ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 179/200\n",
      "1767/1767 [==============================] - 63s 36ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 180/200\n",
      "1767/1767 [==============================] - 63s 36ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 3.0950e-05\n",
      "Epoch 181/200\n",
      "1767/1767 [==============================] - 63s 36ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 182/200\n",
      "1767/1767 [==============================] - 63s 36ms/step - loss: 0.0034 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 0.0000e+00\n",
      "Epoch 183/200\n",
      "1767/1767 [==============================] - 64s 36ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 3.0950e-05\n",
      "Epoch 184/200\n",
      "1767/1767 [==============================] - 64s 36ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 185/200\n",
      "1767/1767 [==============================] - 72s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 3.0950e-05\n",
      "Epoch 186/200\n",
      "1767/1767 [==============================] - 74s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/200\n",
      "1767/1767 [==============================] - 76s 43ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/200\n",
      "1767/1767 [==============================] - 71s 40ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/200\n",
      "1767/1767 [==============================] - 76s 43ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 190/200\n",
      "1767/1767 [==============================] - 68s 38ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0038 - val_accuracy: 3.0950e-05\n",
      "Epoch 191/200\n",
      "1767/1767 [==============================] - 68s 38ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 192/200\n",
      "1767/1767 [==============================] - 67s 38ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0037 - val_accuracy: 0.0000e+00\n",
      "Epoch 193/200\n",
      "1767/1767 [==============================] - 69s 39ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 194/200\n",
      "1767/1767 [==============================] - 69s 39ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/200\n",
      "1767/1767 [==============================] - 77s 43ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/200\n",
      "1767/1767 [==============================] - 71s 40ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 197/200\n",
      "1767/1767 [==============================] - 75s 42ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 198/200\n",
      "1767/1767 [==============================] - 72s 41ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n",
      "Epoch 199/200\n",
      "1767/1767 [==============================] - 81s 46ms/step - loss: 0.0035 - accuracy: 0.0000e+00 - val_loss: 0.0035 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/200\n",
      "1767/1767 [==============================] - 72s 41ms/step - loss: 0.0036 - accuracy: 0.0000e+00 - val_loss: 0.0036 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "rnn_model = rnn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ed52b665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 23s 12ms/step\n",
      "1010/1010 [==============================] - 13s 12ms/step\n",
      "RNN Model Train accuracy: 0.3865505539629056\n",
      "RNN Model Test accuracy: 0.38378839422502886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Make predictions\n",
    "train_preds = rnn_model[0].predict(X_train)\n",
    "test_preds = rnn_model[0].predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, train_preds)\n",
    "test_r2 = r2_score(y_test, test_preds)\n",
    "\n",
    "print(f\"RNN Model Train accuracy: {train_r2}\")\n",
    "print(f\"RNN Model Test accuracy: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe583ada",
   "metadata": {},
   "source": [
    "This RNN model had a training accuracy of 0.385 and testing accuracy of 0.383. The RNN model showed a significant improvement in both training and testing accuracy compared to the previous data science notebook. In the previous notebook, the training accuracy was 0.337 and testing accuracy was 0.292, whereas the RNN model achieved a training accuracy of 0.385 and testing accuracy of 0.383. This performance indicates that the RNN model is able to capture some temporal patterns and dependencies in the data, which is a good sign for time series modeling. There is still room for improvement as the accuracy is not the best. The RNN architecture may need to be modified for better performance. \n",
    "\n",
    "The subsequent RNN model implements a single input layer and a single output layer, utilizing Adam as the optimizer and training for 150 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cdb76f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "def rnn(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.SimpleRNN(units=64, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(layers.Dense(units=1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    # Fit the model to the data\n",
    "    history = model.fit(X_train, y_train, epochs=150, batch_size=32)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8b162790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1767/1767 [==============================] - 25s 13ms/step - loss: 0.0059 - accuracy: 0.0000e+00\n",
      "Epoch 2/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0050 - accuracy: 0.0000e+00\n",
      "Epoch 3/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0049 - accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0048 - accuracy: 0.0000e+00\n",
      "Epoch 5/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0047 - accuracy: 0.0000e+00\n",
      "Epoch 6/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 7/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0045 - accuracy: 0.0000e+00\n",
      "Epoch 8/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 9/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0043 - accuracy: 0.0000e+00\n",
      "Epoch 10/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0043 - accuracy: 0.0000e+00\n",
      "Epoch 11/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0043 - accuracy: 0.0000e+00\n",
      "Epoch 12/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0042 - accuracy: 0.0000e+00\n",
      "Epoch 13/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0042 - accuracy: 0.0000e+00\n",
      "Epoch 14/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 15/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 16/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 17/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 18/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0041 - accuracy: 0.0000e+00\n",
      "Epoch 19/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 20/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0040 - accuracy: 0.0000e+00\n",
      "Epoch 21/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 22/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 23/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 24/150\n",
      "1767/1767 [==============================] - 26s 14ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 25/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 26/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 27/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 28/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 29/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 30/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 31/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 32/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 33/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 34/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 35/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0039 - accuracy: 0.0000e+00\n",
      "Epoch 36/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 37/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 38/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 39/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 40/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 42/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 43/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 44/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 45/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 46/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 47/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 48/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0038 - accuracy: 0.0000e+00\n",
      "Epoch 49/150\n",
      "1767/1767 [==============================] - 24s 14ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 50/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 51/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 52/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 53/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 54/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 55/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 57/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 58/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 59/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 61/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 62/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 63/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 64/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 66/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 67/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 68/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 69/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 70/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 72/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 73/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 74/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 75/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 76/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 77/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 78/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 79/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 80/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 81/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 82/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0037 - accuracy: 0.0000e+00\n",
      "Epoch 83/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 84/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 85/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 86/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 87/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 88/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 89/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 90/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 91/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 92/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 93/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 94/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 95/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 96/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 97/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 98/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 99/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 100/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 101/150\n",
      "1767/1767 [==============================] - 24s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 102/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 103/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 104/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 105/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 106/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 107/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 108/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 109/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 110/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 111/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 112/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 113/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 114/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 115/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 116/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 117/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 118/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 119/150\n",
      "1767/1767 [==============================] - 20s 11ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 120/150\n",
      "1767/1767 [==============================] - 20s 11ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 121/150\n",
      "1767/1767 [==============================] - 21s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 122/150\n",
      "1767/1767 [==============================] - 25s 14ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 123/150\n",
      "1767/1767 [==============================] - 26s 15ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 124/150\n",
      "1767/1767 [==============================] - 27s 15ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 125/150\n",
      "1767/1767 [==============================] - 28s 16ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 126/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 127/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 128/150\n",
      "1767/1767 [==============================] - 19s 11ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 129/150\n",
      "1767/1767 [==============================] - 20s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 130/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 131/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 132/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 133/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 134/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 135/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 136/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 137/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 138/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 139/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 140/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 141/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 142/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 143/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 144/150\n",
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 145/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 146/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 147/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 148/150\n",
      "1767/1767 [==============================] - 23s 13ms/step - loss: 0.0036 - accuracy: 0.0000e+00\n",
      "Epoch 149/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 22s 13ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n",
      "Epoch 150/150\n",
      "1767/1767 [==============================] - 22s 12ms/step - loss: 0.0035 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "rnn_model2 = rnn(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "937dd955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 10s 5ms/step\n",
      "1010/1010 [==============================] - 5s 5ms/step\n",
      "RNN Model Train accuracy: 0.4032004192129188\n",
      "RNN Model Test accuracy: 0.40122144720200714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Make predictions\n",
    "train_preds = rnn_model2[0].predict(X_train)\n",
    "test_preds = rnn_model2[0].predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, train_preds)\n",
    "test_r2 = r2_score(y_test, test_preds)\n",
    "\n",
    "print(f\"RNN Model Train accuracy: {train_r2}\")\n",
    "print(f\"RNN Model Test accuracy: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8405cdf",
   "metadata": {},
   "source": [
    "This RNN model has a training accuracy of 0.403 and a testing accuracy of 0.401. This is an improvement from the previous RNN model. Since this model used fewer layers, it suggests that the simpler architecture performed better due to the reduction in the number of parameters that needed to be learned. This can result in better generalization and less overfitting, allowing the model to better capture the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09b7bd",
   "metadata": {},
   "source": [
    "##### LSTM Model\n",
    "\n",
    "This section includes a LSTM Neural Network Model which includes an input LSTM layer with 50 units, a dropout layer, and one output layer. It is compiled using mean squared error loss and adam optimizer and ran for 200 epochs. \n",
    "\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network that is designed to handle the vanishing gradient problem in traditional RNNs. LSTM networks are used for sequence prediction problems and have the ability to maintain long-term dependencies in the input data. They achieve this by using a memory cell, which allows the network to selectively remember or forget information based on the input. \n",
    "\n",
    "LSTM can be useful in predicting the local marginal price by being able to capture and analyze the long-term dependencies and patterns in the time-series data. The inclusion of dropout layers in the LSTM model can help prevent overfitting and improve generalization to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "bbb636f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3029/3029 - 58s - loss: 0.0080 - 58s/epoch - 19ms/step\n",
      "Epoch 2/200\n",
      "3029/3029 - 51s - loss: 0.0063 - 51s/epoch - 17ms/step\n",
      "Epoch 3/200\n",
      "3029/3029 - 48s - loss: 0.0061 - 48s/epoch - 16ms/step\n",
      "Epoch 4/200\n",
      "3029/3029 - 49s - loss: 0.0059 - 49s/epoch - 16ms/step\n",
      "Epoch 5/200\n",
      "3029/3029 - 65s - loss: 0.0057 - 65s/epoch - 22ms/step\n",
      "Epoch 6/200\n",
      "3029/3029 - 48s - loss: 0.0057 - 48s/epoch - 16ms/step\n",
      "Epoch 7/200\n",
      "3029/3029 - 51s - loss: 0.0056 - 51s/epoch - 17ms/step\n",
      "Epoch 8/200\n",
      "3029/3029 - 49s - loss: 0.0054 - 49s/epoch - 16ms/step\n",
      "Epoch 9/200\n",
      "3029/3029 - 47s - loss: 0.0051 - 47s/epoch - 15ms/step\n",
      "Epoch 10/200\n",
      "3029/3029 - 44s - loss: 0.0049 - 44s/epoch - 15ms/step\n",
      "Epoch 11/200\n",
      "3029/3029 - 43s - loss: 0.0047 - 43s/epoch - 14ms/step\n",
      "Epoch 12/200\n",
      "3029/3029 - 47s - loss: 0.0046 - 47s/epoch - 15ms/step\n",
      "Epoch 13/200\n",
      "3029/3029 - 52s - loss: 0.0045 - 52s/epoch - 17ms/step\n",
      "Epoch 14/200\n",
      "3029/3029 - 50s - loss: 0.0044 - 50s/epoch - 17ms/step\n",
      "Epoch 15/200\n",
      "3029/3029 - 46s - loss: 0.0043 - 46s/epoch - 15ms/step\n",
      "Epoch 16/200\n",
      "3029/3029 - 43s - loss: 0.0042 - 43s/epoch - 14ms/step\n",
      "Epoch 17/200\n",
      "3029/3029 - 43s - loss: 0.0042 - 43s/epoch - 14ms/step\n",
      "Epoch 18/200\n",
      "3029/3029 - 43s - loss: 0.0041 - 43s/epoch - 14ms/step\n",
      "Epoch 19/200\n",
      "3029/3029 - 43s - loss: 0.0041 - 43s/epoch - 14ms/step\n",
      "Epoch 20/200\n",
      "3029/3029 - 43s - loss: 0.0040 - 43s/epoch - 14ms/step\n",
      "Epoch 21/200\n",
      "3029/3029 - 43s - loss: 0.0040 - 43s/epoch - 14ms/step\n",
      "Epoch 22/200\n",
      "3029/3029 - 44s - loss: 0.0040 - 44s/epoch - 14ms/step\n",
      "Epoch 23/200\n",
      "3029/3029 - 51s - loss: 0.0039 - 51s/epoch - 17ms/step\n",
      "Epoch 24/200\n",
      "3029/3029 - 49s - loss: 0.0039 - 49s/epoch - 16ms/step\n",
      "Epoch 25/200\n",
      "3029/3029 - 51s - loss: 0.0039 - 51s/epoch - 17ms/step\n",
      "Epoch 26/200\n",
      "3029/3029 - 49s - loss: 0.0039 - 49s/epoch - 16ms/step\n",
      "Epoch 27/200\n",
      "3029/3029 - 49s - loss: 0.0039 - 49s/epoch - 16ms/step\n",
      "Epoch 28/200\n",
      "3029/3029 - 51s - loss: 0.0039 - 51s/epoch - 17ms/step\n",
      "Epoch 29/200\n",
      "3029/3029 - 50s - loss: 0.0038 - 50s/epoch - 17ms/step\n",
      "Epoch 30/200\n",
      "3029/3029 - 50s - loss: 0.0038 - 50s/epoch - 17ms/step\n",
      "Epoch 31/200\n",
      "3029/3029 - 46s - loss: 0.0038 - 46s/epoch - 15ms/step\n",
      "Epoch 32/200\n",
      "3029/3029 - 52s - loss: 0.0038 - 52s/epoch - 17ms/step\n",
      "Epoch 33/200\n",
      "3029/3029 - 49s - loss: 0.0038 - 49s/epoch - 16ms/step\n",
      "Epoch 34/200\n",
      "3029/3029 - 46s - loss: 0.0038 - 46s/epoch - 15ms/step\n",
      "Epoch 35/200\n",
      "3029/3029 - 50s - loss: 0.0038 - 50s/epoch - 17ms/step\n",
      "Epoch 36/200\n",
      "3029/3029 - 46s - loss: 0.0038 - 46s/epoch - 15ms/step\n",
      "Epoch 37/200\n",
      "3029/3029 - 865s - loss: 0.0038 - 865s/epoch - 286ms/step\n",
      "Epoch 38/200\n",
      "3029/3029 - 46s - loss: 0.0037 - 46s/epoch - 15ms/step\n",
      "Epoch 39/200\n",
      "3029/3029 - 46s - loss: 0.0037 - 46s/epoch - 15ms/step\n",
      "Epoch 40/200\n",
      "3029/3029 - 43s - loss: 0.0037 - 43s/epoch - 14ms/step\n",
      "Epoch 41/200\n",
      "3029/3029 - 43s - loss: 0.0037 - 43s/epoch - 14ms/step\n",
      "Epoch 42/200\n",
      "3029/3029 - 43s - loss: 0.0037 - 43s/epoch - 14ms/step\n",
      "Epoch 43/200\n",
      "3029/3029 - 43s - loss: 0.0037 - 43s/epoch - 14ms/step\n",
      "Epoch 44/200\n",
      "3029/3029 - 50s - loss: 0.0037 - 50s/epoch - 16ms/step\n",
      "Epoch 45/200\n",
      "3029/3029 - 50s - loss: 0.0037 - 50s/epoch - 16ms/step\n",
      "Epoch 46/200\n",
      "3029/3029 - 52s - loss: 0.0037 - 52s/epoch - 17ms/step\n",
      "Epoch 47/200\n",
      "3029/3029 - 49s - loss: 0.0037 - 49s/epoch - 16ms/step\n",
      "Epoch 48/200\n",
      "3029/3029 - 49s - loss: 0.0037 - 49s/epoch - 16ms/step\n",
      "Epoch 49/200\n",
      "3029/3029 - 50s - loss: 0.0037 - 50s/epoch - 16ms/step\n",
      "Epoch 50/200\n",
      "3029/3029 - 48s - loss: 0.0037 - 48s/epoch - 16ms/step\n",
      "Epoch 51/200\n",
      "3029/3029 - 50s - loss: 0.0036 - 50s/epoch - 16ms/step\n",
      "Epoch 52/200\n",
      "3029/3029 - 50s - loss: 0.0037 - 50s/epoch - 16ms/step\n",
      "Epoch 53/200\n",
      "3029/3029 - 51s - loss: 0.0036 - 51s/epoch - 17ms/step\n",
      "Epoch 54/200\n",
      "3029/3029 - 50s - loss: 0.0036 - 50s/epoch - 17ms/step\n",
      "Epoch 55/200\n",
      "3029/3029 - 46s - loss: 0.0036 - 46s/epoch - 15ms/step\n",
      "Epoch 56/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 57/200\n",
      "3029/3029 - 43s - loss: 0.0036 - 43s/epoch - 14ms/step\n",
      "Epoch 58/200\n",
      "3029/3029 - 43s - loss: 0.0036 - 43s/epoch - 14ms/step\n",
      "Epoch 59/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 60/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 61/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 62/200\n",
      "3029/3029 - 43s - loss: 0.0036 - 43s/epoch - 14ms/step\n",
      "Epoch 63/200\n",
      "3029/3029 - 44s - loss: 0.0036 - 44s/epoch - 14ms/step\n",
      "Epoch 64/200\n",
      "3029/3029 - 44s - loss: 0.0036 - 44s/epoch - 14ms/step\n",
      "Epoch 65/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 66/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 67/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 68/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 69/200\n",
      "3029/3029 - 44s - loss: 0.0036 - 44s/epoch - 14ms/step\n",
      "Epoch 70/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 71/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 72/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 73/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 74/200\n",
      "3029/3029 - 42s - loss: 0.0036 - 42s/epoch - 14ms/step\n",
      "Epoch 75/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 76/200\n",
      "3029/3029 - 49s - loss: 0.0035 - 49s/epoch - 16ms/step\n",
      "Epoch 77/200\n",
      "3029/3029 - 45s - loss: 0.0036 - 45s/epoch - 15ms/step\n",
      "Epoch 78/200\n",
      "3029/3029 - 44s - loss: 0.0035 - 44s/epoch - 14ms/step\n",
      "Epoch 79/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 80/200\n",
      "3029/3029 - 43s - loss: 0.0035 - 43s/epoch - 14ms/step\n",
      "Epoch 81/200\n",
      "3029/3029 - 43s - loss: 0.0035 - 43s/epoch - 14ms/step\n",
      "Epoch 82/200\n",
      "3029/3029 - 43s - loss: 0.0035 - 43s/epoch - 14ms/step\n",
      "Epoch 83/200\n",
      "3029/3029 - 44s - loss: 0.0035 - 44s/epoch - 15ms/step\n",
      "Epoch 84/200\n",
      "3029/3029 - 44s - loss: 0.0035 - 44s/epoch - 14ms/step\n",
      "Epoch 85/200\n",
      "3029/3029 - 43s - loss: 0.0035 - 43s/epoch - 14ms/step\n",
      "Epoch 86/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 87/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 88/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 89/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 90/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 91/200\n",
      "3029/3029 - 45s - loss: 0.0035 - 45s/epoch - 15ms/step\n",
      "Epoch 92/200\n",
      "3029/3029 - 45s - loss: 0.0035 - 45s/epoch - 15ms/step\n",
      "Epoch 93/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 94/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 95/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 96/200\n",
      "3029/3029 - 41s - loss: 0.0035 - 41s/epoch - 14ms/step\n",
      "Epoch 97/200\n",
      "3029/3029 - 42s - loss: 0.0035 - 42s/epoch - 14ms/step\n",
      "Epoch 98/200\n",
      "3029/3029 - 57s - loss: 0.0035 - 57s/epoch - 19ms/step\n",
      "Epoch 99/200\n",
      "3029/3029 - 71s - loss: 0.0035 - 71s/epoch - 23ms/step\n",
      "Epoch 100/200\n",
      "3029/3029 - 53s - loss: 0.0035 - 53s/epoch - 18ms/step\n",
      "Epoch 101/200\n",
      "3029/3029 - 55s - loss: 0.0035 - 55s/epoch - 18ms/step\n",
      "Epoch 102/200\n",
      "3029/3029 - 72s - loss: 0.0035 - 72s/epoch - 24ms/step\n",
      "Epoch 103/200\n",
      "3029/3029 - 71s - loss: 0.0035 - 71s/epoch - 23ms/step\n",
      "Epoch 104/200\n",
      "3029/3029 - 78s - loss: 0.0035 - 78s/epoch - 26ms/step\n",
      "Epoch 105/200\n",
      "3029/3029 - 77s - loss: 0.0035 - 77s/epoch - 25ms/step\n",
      "Epoch 106/200\n",
      "3029/3029 - 52s - loss: 0.0035 - 52s/epoch - 17ms/step\n",
      "Epoch 107/200\n",
      "3029/3029 - 55s - loss: 0.0035 - 55s/epoch - 18ms/step\n",
      "Epoch 108/200\n",
      "3029/3029 - 57s - loss: 0.0035 - 57s/epoch - 19ms/step\n",
      "Epoch 109/200\n",
      "3029/3029 - 50s - loss: 0.0034 - 50s/epoch - 17ms/step\n",
      "Epoch 110/200\n",
      "3029/3029 - 47s - loss: 0.0034 - 47s/epoch - 16ms/step\n",
      "Epoch 111/200\n",
      "3029/3029 - 55s - loss: 0.0034 - 55s/epoch - 18ms/step\n",
      "Epoch 112/200\n",
      "3029/3029 - 51s - loss: 0.0035 - 51s/epoch - 17ms/step\n",
      "Epoch 113/200\n",
      "3029/3029 - 53s - loss: 0.0034 - 53s/epoch - 18ms/step\n",
      "Epoch 114/200\n",
      "3029/3029 - 65s - loss: 0.0034 - 65s/epoch - 21ms/step\n",
      "Epoch 115/200\n",
      "3029/3029 - 66s - loss: 0.0034 - 66s/epoch - 22ms/step\n",
      "Epoch 116/200\n",
      "3029/3029 - 52s - loss: 0.0034 - 52s/epoch - 17ms/step\n",
      "Epoch 117/200\n",
      "3029/3029 - 52s - loss: 0.0034 - 52s/epoch - 17ms/step\n",
      "Epoch 118/200\n",
      "3029/3029 - 45s - loss: 0.0034 - 45s/epoch - 15ms/step\n",
      "Epoch 119/200\n",
      "3029/3029 - 54s - loss: 0.0034 - 54s/epoch - 18ms/step\n",
      "Epoch 120/200\n",
      "3029/3029 - 55s - loss: 0.0034 - 55s/epoch - 18ms/step\n",
      "Epoch 121/200\n",
      "3029/3029 - 52s - loss: 0.0034 - 52s/epoch - 17ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 123/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 124/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 125/200\n",
      "3029/3029 - 51s - loss: 0.0034 - 51s/epoch - 17ms/step\n",
      "Epoch 126/200\n",
      "3029/3029 - 51s - loss: 0.0034 - 51s/epoch - 17ms/step\n",
      "Epoch 127/200\n",
      "3029/3029 - 50s - loss: 0.0034 - 50s/epoch - 17ms/step\n",
      "Epoch 128/200\n",
      "3029/3029 - 53s - loss: 0.0034 - 53s/epoch - 17ms/step\n",
      "Epoch 129/200\n",
      "3029/3029 - 52s - loss: 0.0034 - 52s/epoch - 17ms/step\n",
      "Epoch 130/200\n",
      "3029/3029 - 45s - loss: 0.0034 - 45s/epoch - 15ms/step\n",
      "Epoch 131/200\n",
      "3029/3029 - 58s - loss: 0.0034 - 58s/epoch - 19ms/step\n",
      "Epoch 132/200\n",
      "3029/3029 - 54s - loss: 0.0034 - 54s/epoch - 18ms/step\n",
      "Epoch 133/200\n",
      "3029/3029 - 44s - loss: 0.0034 - 44s/epoch - 14ms/step\n",
      "Epoch 134/200\n",
      "3029/3029 - 53s - loss: 0.0034 - 53s/epoch - 18ms/step\n",
      "Epoch 135/200\n",
      "3029/3029 - 44s - loss: 0.0034 - 44s/epoch - 14ms/step\n",
      "Epoch 136/200\n",
      "3029/3029 - 42s - loss: 0.0034 - 42s/epoch - 14ms/step\n",
      "Epoch 137/200\n",
      "3029/3029 - 47s - loss: 0.0034 - 47s/epoch - 15ms/step\n",
      "Epoch 138/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 139/200\n",
      "3029/3029 - 44s - loss: 0.0034 - 44s/epoch - 15ms/step\n",
      "Epoch 140/200\n",
      "3029/3029 - 42s - loss: 0.0034 - 42s/epoch - 14ms/step\n",
      "Epoch 141/200\n",
      "3029/3029 - 45s - loss: 0.0034 - 45s/epoch - 15ms/step\n",
      "Epoch 142/200\n",
      "3029/3029 - 47s - loss: 0.0034 - 47s/epoch - 16ms/step\n",
      "Epoch 143/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 144/200\n",
      "3029/3029 - 59s - loss: 0.0034 - 59s/epoch - 20ms/step\n",
      "Epoch 145/200\n",
      "3029/3029 - 57s - loss: 0.0034 - 57s/epoch - 19ms/step\n",
      "Epoch 146/200\n",
      "3029/3029 - 64s - loss: 0.0034 - 64s/epoch - 21ms/step\n",
      "Epoch 147/200\n",
      "3029/3029 - 67s - loss: 0.0034 - 67s/epoch - 22ms/step\n",
      "Epoch 148/200\n",
      "3029/3029 - 63s - loss: 0.0034 - 63s/epoch - 21ms/step\n",
      "Epoch 149/200\n",
      "3029/3029 - 79s - loss: 0.0034 - 79s/epoch - 26ms/step\n",
      "Epoch 150/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 151/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 152/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 153/200\n",
      "3029/3029 - 42s - loss: 0.0034 - 42s/epoch - 14ms/step\n",
      "Epoch 154/200\n",
      "3029/3029 - 44s - loss: 0.0034 - 44s/epoch - 15ms/step\n",
      "Epoch 155/200\n",
      "3029/3029 - 46s - loss: 0.0034 - 46s/epoch - 15ms/step\n",
      "Epoch 156/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 157/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 158/200\n",
      "3029/3029 - 44s - loss: 0.0034 - 44s/epoch - 14ms/step\n",
      "Epoch 159/200\n",
      "3029/3029 - 44s - loss: 0.0034 - 44s/epoch - 14ms/step\n",
      "Epoch 160/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 161/200\n",
      "3029/3029 - 43s - loss: 0.0034 - 43s/epoch - 14ms/step\n",
      "Epoch 162/200\n",
      "3029/3029 - 47s - loss: 0.0034 - 47s/epoch - 16ms/step\n",
      "Epoch 163/200\n",
      "3029/3029 - 52s - loss: 0.0034 - 52s/epoch - 17ms/step\n",
      "Epoch 164/200\n",
      "3029/3029 - 48s - loss: 0.0034 - 48s/epoch - 16ms/step\n",
      "Epoch 165/200\n",
      "3029/3029 - 49s - loss: 0.0033 - 49s/epoch - 16ms/step\n",
      "Epoch 166/200\n",
      "3029/3029 - 51s - loss: 0.0033 - 51s/epoch - 17ms/step\n",
      "Epoch 167/200\n",
      "3029/3029 - 50s - loss: 0.0033 - 50s/epoch - 17ms/step\n",
      "Epoch 168/200\n",
      "3029/3029 - 52s - loss: 0.0034 - 52s/epoch - 17ms/step\n",
      "Epoch 169/200\n",
      "3029/3029 - 50s - loss: 0.0034 - 50s/epoch - 16ms/step\n",
      "Epoch 170/200\n",
      "3029/3029 - 50s - loss: 0.0033 - 50s/epoch - 16ms/step\n",
      "Epoch 171/200\n",
      "3029/3029 - 52s - loss: 0.0033 - 52s/epoch - 17ms/step\n",
      "Epoch 172/200\n",
      "3029/3029 - 51s - loss: 0.0034 - 51s/epoch - 17ms/step\n",
      "Epoch 173/200\n",
      "3029/3029 - 55s - loss: 0.0034 - 55s/epoch - 18ms/step\n",
      "Epoch 174/200\n",
      "3029/3029 - 52s - loss: 0.0033 - 52s/epoch - 17ms/step\n",
      "Epoch 175/200\n",
      "3029/3029 - 62s - loss: 0.0033 - 62s/epoch - 20ms/step\n",
      "Epoch 176/200\n",
      "3029/3029 - 238s - loss: 0.0033 - 238s/epoch - 79ms/step\n",
      "Epoch 177/200\n",
      "3029/3029 - 48s - loss: 0.0033 - 48s/epoch - 16ms/step\n",
      "Epoch 178/200\n",
      "3029/3029 - 44s - loss: 0.0033 - 44s/epoch - 15ms/step\n",
      "Epoch 179/200\n",
      "3029/3029 - 43s - loss: 0.0033 - 43s/epoch - 14ms/step\n",
      "Epoch 180/200\n",
      "3029/3029 - 42s - loss: 0.0033 - 42s/epoch - 14ms/step\n",
      "Epoch 181/200\n",
      "3029/3029 - 42s - loss: 0.0033 - 42s/epoch - 14ms/step\n",
      "Epoch 182/200\n",
      "3029/3029 - 43s - loss: 0.0033 - 43s/epoch - 14ms/step\n",
      "Epoch 183/200\n",
      "3029/3029 - 50s - loss: 0.0033 - 50s/epoch - 17ms/step\n",
      "Epoch 184/200\n",
      "3029/3029 - 52s - loss: 0.0033 - 52s/epoch - 17ms/step\n",
      "Epoch 185/200\n",
      "3029/3029 - 51s - loss: 0.0033 - 51s/epoch - 17ms/step\n",
      "Epoch 186/200\n",
      "3029/3029 - 50s - loss: 0.0033 - 50s/epoch - 17ms/step\n",
      "Epoch 187/200\n",
      "3029/3029 - 49s - loss: 0.0033 - 49s/epoch - 16ms/step\n",
      "Epoch 188/200\n",
      "3029/3029 - 51s - loss: 0.0033 - 51s/epoch - 17ms/step\n",
      "Epoch 189/200\n",
      "3029/3029 - 49s - loss: 0.0033 - 49s/epoch - 16ms/step\n",
      "Epoch 190/200\n",
      "3029/3029 - 49s - loss: 0.0033 - 49s/epoch - 16ms/step\n",
      "Epoch 191/200\n",
      "3029/3029 - 49s - loss: 0.0033 - 49s/epoch - 16ms/step\n",
      "Epoch 192/200\n",
      "3029/3029 - 49s - loss: 0.0033 - 49s/epoch - 16ms/step\n",
      "Epoch 193/200\n",
      "3029/3029 - 49s - loss: 0.0033 - 49s/epoch - 16ms/step\n",
      "Epoch 194/200\n",
      "3029/3029 - 50s - loss: 0.0033 - 50s/epoch - 17ms/step\n",
      "Epoch 195/200\n",
      "3029/3029 - 61s - loss: 0.0033 - 61s/epoch - 20ms/step\n",
      "Epoch 196/200\n",
      "3029/3029 - 1847s - loss: 0.0033 - 1847s/epoch - 610ms/step\n",
      "Epoch 197/200\n",
      "3029/3029 - 43s - loss: 0.0033 - 43s/epoch - 14ms/step\n",
      "Epoch 198/200\n",
      "3029/3029 - 42s - loss: 0.0033 - 42s/epoch - 14ms/step\n",
      "Epoch 199/200\n",
      "3029/3029 - 170s - loss: 0.0033 - 170s/epoch - 56ms/step\n",
      "Epoch 200/200\n",
      "3029/3029 - 50s - loss: 0.0033 - 50s/epoch - 16ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "def lstm_model(X_train, y_train, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add LSTM layer with 50 units and input shape of (n_steps, n_features)\n",
    "    model.add(LSTM(50, input_shape=(X_train.shape[1], 1)))\n",
    "    \n",
    "    # Add dropout layer to prevent overfitting\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Add output layer with single output value\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(X_train, y_train, epochs=200, batch_size=16, verbose=2)\n",
    "    \n",
    "    return model\n",
    "\n",
    "lstm_model = lstm_model(X_train, y_train, dropout_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "fd8cffa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515/1515 [==============================] - 9s 6ms/step\n",
      "1010/1010 [==============================] - 6s 6ms/step\n",
      "LSTM Model Train accuracy: 0.4552504215593729\n",
      "LSTM Model Test accuracy: 0.4206856353355606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Make predictions\n",
    "train_preds = lstm_model.predict(X_train)\n",
    "test_preds = lstm_model.predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, train_preds)\n",
    "test_r2 = r2_score(y_test, test_preds)\n",
    "\n",
    "print(f\"LSTM Model Train accuracy: {train_r2}\")\n",
    "print(f\"LSTM Model Test accuracy: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa06681",
   "metadata": {},
   "source": [
    "The LSTM Model had a training accuracy of 0.455 and a testing accuracy of 0.421 which is better than both the RNN models and the simple neural network models. This suggests that the LSTM is better able to capture the temporal dependencies in the time series data. This is important in the context of predicting local marginal price, as past prices are likely to be good predictors of future prices. The LSTM's ability to remember past inputs and use that information to inform future predictions makes it a powerful tool for time series forecasting. The LSTM model has demonstrated its effectiveness in predicting the local marginal price. Its ability to handle time-series data makes it a valuable tool in the field of energy economics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaffc23",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network\n",
    "\n",
    "In this section, I will use a convolutional neural network for my time-series data. A convolutional neural network (CNN) is a deep learning model used for image classification tasks but can also be used for time-series data. CNNs apply a series of filters to the input data, which allows the model to detect features in the data. In the context of time-series data, these filters can help identify patterns or trends in the data over time. CNNs can be especially useful for processing long sequences of time-series data, where traditional RNN models may struggle with vanishing gradients. In the context of predicting the local marginal price (LMP), CNN can be useful because it can capture temporal patterns in the data, such as seasonality or trend, while also identifying local features and patterns within each time series. \n",
    "\n",
    "The first CNN model applies several layers to the input data, including convolutional layer, a max pooling layer, a flatten layer, a fully connected layer, and finally an output layer. By using these layers, the model can learn important features in the time series data, and use them to make predictions on future data points. The binary crossentropy loss function and Adam optimizer help the model learn more effectively by adjusting the weights and biases of the network based on the error between predicted and actual values. This approach may lead to more accurate predictions of local marginal prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "098346bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1515/1515 - 9s - loss: 0.6912 - accuracy: 4.1269e-05 - val_loss: 0.6907 - val_accuracy: 0.0000e+00 - 9s/epoch - 6ms/step\n",
      "Epoch 2/200\n",
      "1515/1515 - 6s - loss: 0.6900 - accuracy: 4.1269e-05 - val_loss: 0.6902 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 3/200\n",
      "1515/1515 - 13s - loss: 0.6894 - accuracy: 4.1269e-05 - val_loss: 0.6896 - val_accuracy: 0.0000e+00 - 13s/epoch - 9ms/step\n",
      "Epoch 4/200\n",
      "1515/1515 - 13s - loss: 0.6891 - accuracy: 4.1269e-05 - val_loss: 0.6895 - val_accuracy: 0.0000e+00 - 13s/epoch - 8ms/step\n",
      "Epoch 5/200\n",
      "1515/1515 - 18s - loss: 0.6889 - accuracy: 4.1269e-05 - val_loss: 0.6893 - val_accuracy: 0.0000e+00 - 18s/epoch - 12ms/step\n",
      "Epoch 6/200\n",
      "1515/1515 - 19s - loss: 0.6888 - accuracy: 2.0634e-05 - val_loss: 0.6894 - val_accuracy: 0.0000e+00 - 19s/epoch - 13ms/step\n",
      "Epoch 7/200\n",
      "1515/1515 - 11s - loss: 0.6887 - accuracy: 2.0634e-05 - val_loss: 0.6890 - val_accuracy: 0.0000e+00 - 11s/epoch - 7ms/step\n",
      "Epoch 8/200\n",
      "1515/1515 - 6s - loss: 0.6887 - accuracy: 2.0634e-05 - val_loss: 0.6891 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 9/200\n",
      "1515/1515 - 6s - loss: 0.6886 - accuracy: 2.0634e-05 - val_loss: 0.6893 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 10/200\n",
      "1515/1515 - 6s - loss: 0.6886 - accuracy: 2.0634e-05 - val_loss: 0.6890 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 11/200\n",
      "1515/1515 - 6s - loss: 0.6885 - accuracy: 2.0634e-05 - val_loss: 0.6892 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 12/200\n",
      "1515/1515 - 6s - loss: 0.6885 - accuracy: 2.0634e-05 - val_loss: 0.6891 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 13/200\n",
      "1515/1515 - 6s - loss: 0.6884 - accuracy: 2.0634e-05 - val_loss: 0.6888 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 14/200\n",
      "1515/1515 - 6s - loss: 0.6884 - accuracy: 2.0634e-05 - val_loss: 0.6890 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 15/200\n",
      "1515/1515 - 6s - loss: 0.6884 - accuracy: 2.0634e-05 - val_loss: 0.6892 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 16/200\n",
      "1515/1515 - 7s - loss: 0.6883 - accuracy: 2.0634e-05 - val_loss: 0.6888 - val_accuracy: 0.0000e+00 - 7s/epoch - 4ms/step\n",
      "Epoch 17/200\n",
      "1515/1515 - 6s - loss: 0.6883 - accuracy: 4.1269e-05 - val_loss: 0.6887 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 18/200\n",
      "1515/1515 - 6s - loss: 0.6883 - accuracy: 2.0634e-05 - val_loss: 0.6888 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 19/200\n",
      "1515/1515 - 6s - loss: 0.6883 - accuracy: 2.0634e-05 - val_loss: 0.6889 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 20/200\n",
      "1515/1515 - 9s - loss: 0.6882 - accuracy: 2.0634e-05 - val_loss: 0.6887 - val_accuracy: 0.0000e+00 - 9s/epoch - 6ms/step\n",
      "Epoch 21/200\n",
      "1515/1515 - 6s - loss: 0.6882 - accuracy: 2.0634e-05 - val_loss: 0.6887 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 22/200\n",
      "1515/1515 - 6s - loss: 0.6882 - accuracy: 2.0634e-05 - val_loss: 0.6887 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 23/200\n",
      "1515/1515 - 6s - loss: 0.6882 - accuracy: 2.0634e-05 - val_loss: 0.6888 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 24/200\n",
      "1515/1515 - 6s - loss: 0.6882 - accuracy: 2.0634e-05 - val_loss: 0.6890 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 25/200\n",
      "1515/1515 - 6s - loss: 0.6882 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 26/200\n",
      "1515/1515 - 6s - loss: 0.6882 - accuracy: 2.0634e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 27/200\n",
      "1515/1515 - 6s - loss: 0.6881 - accuracy: 4.1269e-05 - val_loss: 0.6887 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 28/200\n",
      "1515/1515 - 6s - loss: 0.6881 - accuracy: 2.0634e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 29/200\n",
      "1515/1515 - 6s - loss: 0.6881 - accuracy: 2.0634e-05 - val_loss: 0.6888 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 30/200\n",
      "1515/1515 - 6s - loss: 0.6881 - accuracy: 2.0634e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 31/200\n",
      "1515/1515 - 6s - loss: 0.6881 - accuracy: 2.0634e-05 - val_loss: 0.6891 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 32/200\n",
      "1515/1515 - 6s - loss: 0.6881 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 33/200\n",
      "1515/1515 - 6s - loss: 0.6881 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 34/200\n",
      "1515/1515 - 6s - loss: 0.6881 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 35/200\n",
      "1515/1515 - 6s - loss: 0.6881 - accuracy: 4.1269e-05 - val_loss: 0.6887 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 36/200\n",
      "1515/1515 - 6s - loss: 0.6880 - accuracy: 2.0634e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 37/200\n",
      "1515/1515 - 6s - loss: 0.6880 - accuracy: 2.0634e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 38/200\n",
      "1515/1515 - 6s - loss: 0.6881 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 39/200\n",
      "1515/1515 - 6s - loss: 0.6880 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 40/200\n",
      "1515/1515 - 6s - loss: 0.6880 - accuracy: 2.0634e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 41/200\n",
      "1515/1515 - 6s - loss: 0.6880 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 42/200\n",
      "1515/1515 - 5s - loss: 0.6880 - accuracy: 4.1269e-05 - val_loss: 0.6888 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 43/200\n",
      "1515/1515 - 5s - loss: 0.6880 - accuracy: 2.0634e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 44/200\n",
      "1515/1515 - 7s - loss: 0.6880 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 7s/epoch - 4ms/step\n",
      "Epoch 45/200\n",
      "1515/1515 - 6s - loss: 0.6880 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 46/200\n",
      "1515/1515 - 6s - loss: 0.6880 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 47/200\n",
      "1515/1515 - 6s - loss: 0.6880 - accuracy: 4.1269e-05 - val_loss: 0.6887 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 48/200\n",
      "1515/1515 - 5s - loss: 0.6880 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 49/200\n",
      "1515/1515 - 6s - loss: 0.6880 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 50/200\n",
      "1515/1515 - 6s - loss: 0.6880 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 51/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 52/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 53/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 54/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 2.0634e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 55/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 2.0634e-05 - val_loss: 0.6887 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 56/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 57/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 58/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 59/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 60/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 61/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 2.0634e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 62/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 2.0634e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 64/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 65/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 66/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 67/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 68/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 69/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 70/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 71/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 72/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 73/200\n",
      "1515/1515 - 5s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 74/200\n",
      "1515/1515 - 7s - loss: 0.6879 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 7s/epoch - 5ms/step\n",
      "Epoch 75/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 76/200\n",
      "1515/1515 - 6s - loss: 0.6879 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 77/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 78/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 79/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 80/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 81/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 82/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6887 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 83/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 2.0634e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 84/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 85/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 86/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 2.0634e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 87/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 88/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 89/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 90/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 91/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 92/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 93/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 94/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 95/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 96/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 97/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 2.0634e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 98/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 99/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 100/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 101/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 2.0634e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 102/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 103/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 104/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 105/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 106/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 107/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 108/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 2.0634e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 109/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 110/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 111/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 112/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 113/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 114/200\n",
      "1515/1515 - 6s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 115/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 116/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 117/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 118/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 119/200\n",
      "1515/1515 - 5s - loss: 0.6878 - accuracy: 2.0634e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 120/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 121/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 122/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 123/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 125/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 126/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 127/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 128/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 129/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 130/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 131/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 132/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 133/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 134/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 135/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 136/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 137/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 138/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 139/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 140/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 141/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 142/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 143/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 144/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 145/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 146/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 147/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 2.0634e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 3ms/step\n",
      "Epoch 148/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 149/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 150/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 151/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 152/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 153/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 154/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 155/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 156/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 2.0634e-05 - val_loss: 0.6882 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 157/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 158/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6886 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 159/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 2.0634e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 160/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 161/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 162/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 163/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 164/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 165/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 2.0634e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 166/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6882 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 167/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 168/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 169/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 170/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 171/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 172/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 173/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 174/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 175/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 176/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 177/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 178/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 179/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 2.0634e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 180/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 181/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 182/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 183/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 184/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 186/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6885 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 187/200\n",
      "1515/1515 - 5s - loss: 0.6876 - accuracy: 4.1269e-05 - val_loss: 0.6882 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 188/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 189/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 190/200\n",
      "1515/1515 - 5s - loss: 0.6876 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n",
      "Epoch 191/200\n",
      "1515/1515 - 6s - loss: 0.6876 - accuracy: 4.1269e-05 - val_loss: 0.6882 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 192/200\n",
      "1515/1515 - 5s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 3ms/step\n",
      "Epoch 193/200\n",
      "1515/1515 - 5s - loss: 0.6876 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 5s/epoch - 3ms/step\n",
      "Epoch 194/200\n",
      "1515/1515 - 6s - loss: 0.6876 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 195/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 196/200\n",
      "1515/1515 - 6s - loss: 0.6876 - accuracy: 4.1269e-05 - val_loss: 0.6882 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 197/200\n",
      "1515/1515 - 6s - loss: 0.6877 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 198/200\n",
      "1515/1515 - 6s - loss: 0.6876 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 199/200\n",
      "1515/1515 - 6s - loss: 0.6876 - accuracy: 4.1269e-05 - val_loss: 0.6884 - val_accuracy: 0.0000e+00 - 6s/epoch - 4ms/step\n",
      "Epoch 200/200\n",
      "1515/1515 - 5s - loss: 0.6876 - accuracy: 4.1269e-05 - val_loss: 0.6883 - val_accuracy: 0.0000e+00 - 5s/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fde1553ff40>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "conv_model = Sequential()\n",
    "\n",
    "# Add a 1D convolutional layer with 32 filters, kernel size of 3 and ReLU activation\n",
    "conv_model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "\n",
    "# Add a max pooling layer\n",
    "conv_model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten the output\n",
    "conv_model.add(Flatten())\n",
    "\n",
    "# Add a fully connected layer with 100 units and ReLU activation\n",
    "conv_model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add the output layer with sigmoid activation for binary classification\n",
    "conv_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary cross-entropy loss and Adam optimizer\n",
    "conv_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "conv_model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test), verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3d1a6431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515/1515 [==============================] - 4s 2ms/step\n",
      "1010/1010 [==============================] - 2s 2ms/step\n",
      "Convolutional Neural Network Train accuracy: 0.45787733848172296\n",
      "Convolutional Neural Network Test accuracy: 0.42529556994447715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Make predictions\n",
    "train_preds = conv_model.predict(X_train)\n",
    "test_preds = conv_model.predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, train_preds)\n",
    "test_r2 = r2_score(y_test, test_preds)\n",
    "\n",
    "print(f\"Convolutional Neural Network Train accuracy: {train_r2}\")\n",
    "print(f\"Convolutional Neural Network Test accuracy: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03343f86",
   "metadata": {},
   "source": [
    "This CNN model had a training accuracy of 0.458 and a testing accuracy of 0.425. This is an improvement from both the RNN models seen previously. This improvement in accuracy suggests that the CNN model is better suited for predicting the local marginal price compared to the RNN models used previously. CNNs are particularly useful for identifying patterns in time-series data, which is essential in predicting future values of the local marginal price. The convolutional layers of the CNN are able to identify relevant features in the time series data, while the max pooling layer reduces the dimensionality of the data, making it easier for the fully connected layer to make predictions. The binary crossentropy loss and Adam optimizer used in this model further improve its performance.\n",
    "\n",
    "The next CNN model has 1 convolutional input layer, 2 max pooling layers, a hidden convolutional layer, a flatten layer, a densely connected layer, and finally an output layer. The model is compiled using MAE loss and adam optimizer and ran for 200 epochs. The addition of more layers and the use of MAE loss instead of binary crossentropy loss in this CNN model suggest a different approach to handling the time-series data for predicting the local marginal price. With more layers, the model is able to capture more complex patterns and features in the data. Using MAE loss instead of binary crossentropy loss emphasizes the accuracy of the model's predictions, rather than the classification accuracy. With 200 epochs, the model has more opportunities to adjust its weights and improve its performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "99240483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "674/674 - 7s - loss: 0.0696 - val_loss: 0.0602 - 7s/epoch - 10ms/step\n",
      "Epoch 2/200\n",
      "674/674 - 5s - loss: 0.0586 - val_loss: 0.0524 - 5s/epoch - 7ms/step\n",
      "Epoch 3/200\n",
      "674/674 - 6s - loss: 0.0572 - val_loss: 0.0525 - 6s/epoch - 8ms/step\n",
      "Epoch 4/200\n",
      "674/674 - 7s - loss: 0.0557 - val_loss: 0.0538 - 7s/epoch - 10ms/step\n",
      "Epoch 5/200\n",
      "674/674 - 5s - loss: 0.0548 - val_loss: 0.0524 - 5s/epoch - 8ms/step\n",
      "Epoch 6/200\n",
      "674/674 - 5s - loss: 0.0538 - val_loss: 0.0523 - 5s/epoch - 8ms/step\n",
      "Epoch 7/200\n",
      "674/674 - 6s - loss: 0.0537 - val_loss: 0.0507 - 6s/epoch - 8ms/step\n",
      "Epoch 8/200\n",
      "674/674 - 5s - loss: 0.0527 - val_loss: 0.0491 - 5s/epoch - 8ms/step\n",
      "Epoch 9/200\n",
      "674/674 - 5s - loss: 0.0527 - val_loss: 0.0501 - 5s/epoch - 8ms/step\n",
      "Epoch 10/200\n",
      "674/674 - 5s - loss: 0.0522 - val_loss: 0.0503 - 5s/epoch - 7ms/step\n",
      "Epoch 11/200\n",
      "674/674 - 5s - loss: 0.0519 - val_loss: 0.0495 - 5s/epoch - 7ms/step\n",
      "Epoch 12/200\n",
      "674/674 - 5s - loss: 0.0516 - val_loss: 0.0498 - 5s/epoch - 7ms/step\n",
      "Epoch 13/200\n",
      "674/674 - 5s - loss: 0.0510 - val_loss: 0.0492 - 5s/epoch - 7ms/step\n",
      "Epoch 14/200\n",
      "674/674 - 5s - loss: 0.0504 - val_loss: 0.0484 - 5s/epoch - 7ms/step\n",
      "Epoch 15/200\n",
      "674/674 - 5s - loss: 0.0501 - val_loss: 0.0486 - 5s/epoch - 7ms/step\n",
      "Epoch 16/200\n",
      "674/674 - 5s - loss: 0.0497 - val_loss: 0.0483 - 5s/epoch - 7ms/step\n",
      "Epoch 17/200\n",
      "674/674 - 5s - loss: 0.0493 - val_loss: 0.0483 - 5s/epoch - 8ms/step\n",
      "Epoch 18/200\n",
      "674/674 - 5s - loss: 0.0490 - val_loss: 0.0482 - 5s/epoch - 7ms/step\n",
      "Epoch 19/200\n",
      "674/674 - 5s - loss: 0.0489 - val_loss: 0.0480 - 5s/epoch - 7ms/step\n",
      "Epoch 20/200\n",
      "674/674 - 5s - loss: 0.0484 - val_loss: 0.0473 - 5s/epoch - 7ms/step\n",
      "Epoch 21/200\n",
      "674/674 - 5s - loss: 0.0482 - val_loss: 0.0483 - 5s/epoch - 7ms/step\n",
      "Epoch 22/200\n",
      "674/674 - 5s - loss: 0.0479 - val_loss: 0.0478 - 5s/epoch - 7ms/step\n",
      "Epoch 23/200\n",
      "674/674 - 5s - loss: 0.0477 - val_loss: 0.0474 - 5s/epoch - 7ms/step\n",
      "Epoch 24/200\n",
      "674/674 - 5s - loss: 0.0477 - val_loss: 0.0475 - 5s/epoch - 7ms/step\n",
      "Epoch 25/200\n",
      "674/674 - 5s - loss: 0.0475 - val_loss: 0.0475 - 5s/epoch - 7ms/step\n",
      "Epoch 26/200\n",
      "674/674 - 5s - loss: 0.0473 - val_loss: 0.0470 - 5s/epoch - 7ms/step\n",
      "Epoch 27/200\n",
      "674/674 - 5s - loss: 0.0470 - val_loss: 0.0463 - 5s/epoch - 7ms/step\n",
      "Epoch 28/200\n",
      "674/674 - 5s - loss: 0.0471 - val_loss: 0.0465 - 5s/epoch - 7ms/step\n",
      "Epoch 29/200\n",
      "674/674 - 5s - loss: 0.0467 - val_loss: 0.0463 - 5s/epoch - 7ms/step\n",
      "Epoch 30/200\n",
      "674/674 - 5s - loss: 0.0464 - val_loss: 0.0468 - 5s/epoch - 7ms/step\n",
      "Epoch 31/200\n",
      "674/674 - 4s - loss: 0.0463 - val_loss: 0.0460 - 4s/epoch - 7ms/step\n",
      "Epoch 32/200\n",
      "674/674 - 6s - loss: 0.0463 - val_loss: 0.0459 - 6s/epoch - 8ms/step\n",
      "Epoch 33/200\n",
      "674/674 - 5s - loss: 0.0461 - val_loss: 0.0452 - 5s/epoch - 7ms/step\n",
      "Epoch 34/200\n",
      "674/674 - 5s - loss: 0.0460 - val_loss: 0.0452 - 5s/epoch - 7ms/step\n",
      "Epoch 35/200\n",
      "674/674 - 5s - loss: 0.0458 - val_loss: 0.0456 - 5s/epoch - 7ms/step\n",
      "Epoch 36/200\n",
      "674/674 - 5s - loss: 0.0456 - val_loss: 0.0451 - 5s/epoch - 7ms/step\n",
      "Epoch 37/200\n",
      "674/674 - 5s - loss: 0.0457 - val_loss: 0.0449 - 5s/epoch - 7ms/step\n",
      "Epoch 38/200\n",
      "674/674 - 5s - loss: 0.0455 - val_loss: 0.0451 - 5s/epoch - 7ms/step\n",
      "Epoch 39/200\n",
      "674/674 - 5s - loss: 0.0454 - val_loss: 0.0457 - 5s/epoch - 7ms/step\n",
      "Epoch 40/200\n",
      "674/674 - 5s - loss: 0.0454 - val_loss: 0.0451 - 5s/epoch - 7ms/step\n",
      "Epoch 41/200\n",
      "674/674 - 5s - loss: 0.0454 - val_loss: 0.0450 - 5s/epoch - 7ms/step\n",
      "Epoch 42/200\n",
      "674/674 - 5s - loss: 0.0453 - val_loss: 0.0455 - 5s/epoch - 7ms/step\n",
      "Epoch 43/200\n",
      "674/674 - 5s - loss: 0.0451 - val_loss: 0.0455 - 5s/epoch - 7ms/step\n",
      "Epoch 44/200\n",
      "674/674 - 5s - loss: 0.0451 - val_loss: 0.0453 - 5s/epoch - 7ms/step\n",
      "Epoch 45/200\n",
      "674/674 - 5s - loss: 0.0450 - val_loss: 0.0454 - 5s/epoch - 7ms/step\n",
      "Epoch 46/200\n",
      "674/674 - 5s - loss: 0.0450 - val_loss: 0.0449 - 5s/epoch - 7ms/step\n",
      "Epoch 47/200\n",
      "674/674 - 4s - loss: 0.0450 - val_loss: 0.0449 - 4s/epoch - 7ms/step\n",
      "Epoch 48/200\n",
      "674/674 - 5s - loss: 0.0448 - val_loss: 0.0453 - 5s/epoch - 7ms/step\n",
      "Epoch 49/200\n",
      "674/674 - 5s - loss: 0.0448 - val_loss: 0.0451 - 5s/epoch - 7ms/step\n",
      "Epoch 50/200\n",
      "674/674 - 5s - loss: 0.0448 - val_loss: 0.0454 - 5s/epoch - 7ms/step\n",
      "Epoch 51/200\n",
      "674/674 - 5s - loss: 0.0447 - val_loss: 0.0456 - 5s/epoch - 8ms/step\n",
      "Epoch 52/200\n",
      "674/674 - 5s - loss: 0.0447 - val_loss: 0.0451 - 5s/epoch - 7ms/step\n",
      "Epoch 53/200\n",
      "674/674 - 5s - loss: 0.0446 - val_loss: 0.0454 - 5s/epoch - 7ms/step\n",
      "Epoch 54/200\n",
      "674/674 - 5s - loss: 0.0447 - val_loss: 0.0451 - 5s/epoch - 7ms/step\n",
      "Epoch 55/200\n",
      "674/674 - 5s - loss: 0.0445 - val_loss: 0.0450 - 5s/epoch - 8ms/step\n",
      "Epoch 56/200\n",
      "674/674 - 5s - loss: 0.0445 - val_loss: 0.0455 - 5s/epoch - 7ms/step\n",
      "Epoch 57/200\n",
      "674/674 - 5s - loss: 0.0444 - val_loss: 0.0446 - 5s/epoch - 7ms/step\n",
      "Epoch 58/200\n",
      "674/674 - 5s - loss: 0.0443 - val_loss: 0.0446 - 5s/epoch - 7ms/step\n",
      "Epoch 59/200\n",
      "674/674 - 5s - loss: 0.0443 - val_loss: 0.0444 - 5s/epoch - 7ms/step\n",
      "Epoch 60/200\n",
      "674/674 - 6s - loss: 0.0444 - val_loss: 0.0448 - 6s/epoch - 8ms/step\n",
      "Epoch 61/200\n",
      "674/674 - 6s - loss: 0.0442 - val_loss: 0.0443 - 6s/epoch - 9ms/step\n",
      "Epoch 62/200\n",
      "674/674 - 5s - loss: 0.0442 - val_loss: 0.0444 - 5s/epoch - 8ms/step\n",
      "Epoch 63/200\n",
      "674/674 - 5s - loss: 0.0442 - val_loss: 0.0444 - 5s/epoch - 8ms/step\n",
      "Epoch 64/200\n",
      "674/674 - 5s - loss: 0.0440 - val_loss: 0.0442 - 5s/epoch - 8ms/step\n",
      "Epoch 65/200\n",
      "674/674 - 5s - loss: 0.0441 - val_loss: 0.0446 - 5s/epoch - 7ms/step\n",
      "Epoch 66/200\n",
      "674/674 - 5s - loss: 0.0441 - val_loss: 0.0442 - 5s/epoch - 7ms/step\n",
      "Epoch 67/200\n",
      "674/674 - 5s - loss: 0.0440 - val_loss: 0.0445 - 5s/epoch - 7ms/step\n",
      "Epoch 68/200\n",
      "674/674 - 4s - loss: 0.0439 - val_loss: 0.0442 - 4s/epoch - 7ms/step\n",
      "Epoch 69/200\n",
      "674/674 - 4s - loss: 0.0439 - val_loss: 0.0441 - 4s/epoch - 7ms/step\n",
      "Epoch 70/200\n",
      "674/674 - 5s - loss: 0.0439 - val_loss: 0.0442 - 5s/epoch - 8ms/step\n",
      "Epoch 71/200\n",
      "674/674 - 5s - loss: 0.0439 - val_loss: 0.0442 - 5s/epoch - 7ms/step\n",
      "Epoch 72/200\n",
      "674/674 - 7s - loss: 0.0437 - val_loss: 0.0442 - 7s/epoch - 10ms/step\n",
      "Epoch 73/200\n",
      "674/674 - 7s - loss: 0.0437 - val_loss: 0.0446 - 7s/epoch - 10ms/step\n",
      "Epoch 74/200\n",
      "674/674 - 5s - loss: 0.0439 - val_loss: 0.0442 - 5s/epoch - 7ms/step\n",
      "Epoch 75/200\n",
      "674/674 - 5s - loss: 0.0437 - val_loss: 0.0443 - 5s/epoch - 7ms/step\n",
      "Epoch 76/200\n",
      "674/674 - 5s - loss: 0.0438 - val_loss: 0.0441 - 5s/epoch - 7ms/step\n",
      "Epoch 77/200\n",
      "674/674 - 5s - loss: 0.0438 - val_loss: 0.0444 - 5s/epoch - 7ms/step\n",
      "Epoch 78/200\n",
      "674/674 - 5s - loss: 0.0437 - val_loss: 0.0444 - 5s/epoch - 7ms/step\n",
      "Epoch 79/200\n",
      "674/674 - 5s - loss: 0.0436 - val_loss: 0.0432 - 5s/epoch - 7ms/step\n",
      "Epoch 80/200\n",
      "674/674 - 5s - loss: 0.0437 - val_loss: 0.0444 - 5s/epoch - 7ms/step\n",
      "Epoch 81/200\n",
      "674/674 - 5s - loss: 0.0436 - val_loss: 0.0440 - 5s/epoch - 7ms/step\n",
      "Epoch 82/200\n",
      "674/674 - 5s - loss: 0.0436 - val_loss: 0.0440 - 5s/epoch - 7ms/step\n",
      "Epoch 83/200\n",
      "674/674 - 5s - loss: 0.0435 - val_loss: 0.0442 - 5s/epoch - 8ms/step\n",
      "Epoch 84/200\n",
      "674/674 - 5s - loss: 0.0435 - val_loss: 0.0440 - 5s/epoch - 8ms/step\n",
      "Epoch 85/200\n",
      "674/674 - 5s - loss: 0.0435 - val_loss: 0.0445 - 5s/epoch - 7ms/step\n",
      "Epoch 86/200\n",
      "674/674 - 5s - loss: 0.0436 - val_loss: 0.0441 - 5s/epoch - 7ms/step\n",
      "Epoch 87/200\n",
      "674/674 - 5s - loss: 0.0435 - val_loss: 0.0440 - 5s/epoch - 7ms/step\n",
      "Epoch 88/200\n",
      "674/674 - 5s - loss: 0.0434 - val_loss: 0.0439 - 5s/epoch - 7ms/step\n",
      "Epoch 89/200\n",
      "674/674 - 5s - loss: 0.0435 - val_loss: 0.0440 - 5s/epoch - 7ms/step\n",
      "Epoch 90/200\n",
      "674/674 - 5s - loss: 0.0435 - val_loss: 0.0439 - 5s/epoch - 7ms/step\n",
      "Epoch 91/200\n",
      "674/674 - 5s - loss: 0.0435 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 92/200\n",
      "674/674 - 5s - loss: 0.0434 - val_loss: 0.0440 - 5s/epoch - 7ms/step\n",
      "Epoch 93/200\n",
      "674/674 - 5s - loss: 0.0434 - val_loss: 0.0437 - 5s/epoch - 7ms/step\n",
      "Epoch 94/200\n",
      "674/674 - 6s - loss: 0.0433 - val_loss: 0.0440 - 6s/epoch - 9ms/step\n",
      "Epoch 95/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0436 - 5s/epoch - 8ms/step\n",
      "Epoch 96/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0440 - 5s/epoch - 7ms/step\n",
      "Epoch 97/200\n",
      "674/674 - 6s - loss: 0.0434 - val_loss: 0.0436 - 6s/epoch - 8ms/step\n",
      "Epoch 98/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0439 - 5s/epoch - 7ms/step\n",
      "Epoch 99/200\n",
      "674/674 - 5s - loss: 0.0434 - val_loss: 0.0438 - 5s/epoch - 7ms/step\n",
      "Epoch 100/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0439 - 5s/epoch - 7ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200\n",
      "674/674 - 5s - loss: 0.0434 - val_loss: 0.0437 - 5s/epoch - 7ms/step\n",
      "Epoch 102/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0435 - 5s/epoch - 7ms/step\n",
      "Epoch 103/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0438 - 5s/epoch - 7ms/step\n",
      "Epoch 104/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0435 - 5s/epoch - 7ms/step\n",
      "Epoch 105/200\n",
      "674/674 - 4s - loss: 0.0433 - val_loss: 0.0434 - 4s/epoch - 7ms/step\n",
      "Epoch 106/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0433 - 5s/epoch - 7ms/step\n",
      "Epoch 107/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0433 - 5s/epoch - 7ms/step\n",
      "Epoch 108/200\n",
      "674/674 - 5s - loss: 0.0434 - val_loss: 0.0437 - 5s/epoch - 7ms/step\n",
      "Epoch 109/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0437 - 5s/epoch - 8ms/step\n",
      "Epoch 110/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0442 - 5s/epoch - 7ms/step\n",
      "Epoch 111/200\n",
      "674/674 - 5s - loss: 0.0433 - val_loss: 0.0434 - 5s/epoch - 7ms/step\n",
      "Epoch 112/200\n",
      "674/674 - 5s - loss: 0.0432 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 113/200\n",
      "674/674 - 5s - loss: 0.0431 - val_loss: 0.0440 - 5s/epoch - 7ms/step\n",
      "Epoch 114/200\n",
      "674/674 - 4s - loss: 0.0433 - val_loss: 0.0437 - 4s/epoch - 7ms/step\n",
      "Epoch 115/200\n",
      "674/674 - 5s - loss: 0.0431 - val_loss: 0.0435 - 5s/epoch - 7ms/step\n",
      "Epoch 116/200\n",
      "674/674 - 6s - loss: 0.0431 - val_loss: 0.0439 - 6s/epoch - 9ms/step\n",
      "Epoch 117/200\n",
      "674/674 - 5s - loss: 0.0431 - val_loss: 0.0439 - 5s/epoch - 7ms/step\n",
      "Epoch 118/200\n",
      "674/674 - 5s - loss: 0.0431 - val_loss: 0.0438 - 5s/epoch - 7ms/step\n",
      "Epoch 119/200\n",
      "674/674 - 5s - loss: 0.0431 - val_loss: 0.0442 - 5s/epoch - 7ms/step\n",
      "Epoch 120/200\n",
      "674/674 - 5s - loss: 0.0431 - val_loss: 0.0438 - 5s/epoch - 7ms/step\n",
      "Epoch 121/200\n",
      "674/674 - 5s - loss: 0.0430 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 122/200\n",
      "674/674 - 5s - loss: 0.0430 - val_loss: 0.0438 - 5s/epoch - 8ms/step\n",
      "Epoch 123/200\n",
      "674/674 - 5s - loss: 0.0429 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 124/200\n",
      "674/674 - 4s - loss: 0.0429 - val_loss: 0.0437 - 4s/epoch - 7ms/step\n",
      "Epoch 125/200\n",
      "674/674 - 4s - loss: 0.0428 - val_loss: 0.0435 - 4s/epoch - 7ms/step\n",
      "Epoch 126/200\n",
      "674/674 - 5s - loss: 0.0428 - val_loss: 0.0437 - 5s/epoch - 7ms/step\n",
      "Epoch 127/200\n",
      "674/674 - 5s - loss: 0.0429 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 128/200\n",
      "674/674 - 5s - loss: 0.0428 - val_loss: 0.0434 - 5s/epoch - 7ms/step\n",
      "Epoch 129/200\n",
      "674/674 - 5s - loss: 0.0429 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 130/200\n",
      "674/674 - 5s - loss: 0.0429 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 131/200\n",
      "674/674 - 5s - loss: 0.0428 - val_loss: 0.0435 - 5s/epoch - 7ms/step\n",
      "Epoch 132/200\n",
      "674/674 - 5s - loss: 0.0428 - val_loss: 0.0439 - 5s/epoch - 7ms/step\n",
      "Epoch 133/200\n",
      "674/674 - 5s - loss: 0.0428 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 134/200\n",
      "674/674 - 5s - loss: 0.0427 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 135/200\n",
      "674/674 - 4s - loss: 0.0427 - val_loss: 0.0437 - 4s/epoch - 7ms/step\n",
      "Epoch 136/200\n",
      "674/674 - 5s - loss: 0.0427 - val_loss: 0.0435 - 5s/epoch - 8ms/step\n",
      "Epoch 137/200\n",
      "674/674 - 5s - loss: 0.0427 - val_loss: 0.0433 - 5s/epoch - 8ms/step\n",
      "Epoch 138/200\n",
      "674/674 - 6s - loss: 0.0427 - val_loss: 0.0439 - 6s/epoch - 8ms/step\n",
      "Epoch 139/200\n",
      "674/674 - 5s - loss: 0.0427 - val_loss: 0.0433 - 5s/epoch - 8ms/step\n",
      "Epoch 140/200\n",
      "674/674 - 5s - loss: 0.0427 - val_loss: 0.0434 - 5s/epoch - 7ms/step\n",
      "Epoch 141/200\n",
      "674/674 - 5s - loss: 0.0427 - val_loss: 0.0434 - 5s/epoch - 8ms/step\n",
      "Epoch 142/200\n",
      "674/674 - 5s - loss: 0.0428 - val_loss: 0.0435 - 5s/epoch - 8ms/step\n",
      "Epoch 143/200\n",
      "674/674 - 6s - loss: 0.0427 - val_loss: 0.0437 - 6s/epoch - 8ms/step\n",
      "Epoch 144/200\n",
      "674/674 - 6s - loss: 0.0427 - val_loss: 0.0434 - 6s/epoch - 8ms/step\n",
      "Epoch 145/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0433 - 5s/epoch - 8ms/step\n",
      "Epoch 146/200\n",
      "674/674 - 6s - loss: 0.0426 - val_loss: 0.0433 - 6s/epoch - 8ms/step\n",
      "Epoch 147/200\n",
      "674/674 - 6s - loss: 0.0426 - val_loss: 0.0432 - 6s/epoch - 8ms/step\n",
      "Epoch 148/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0431 - 5s/epoch - 8ms/step\n",
      "Epoch 149/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0431 - 5s/epoch - 7ms/step\n",
      "Epoch 150/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0431 - 5s/epoch - 7ms/step\n",
      "Epoch 151/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0431 - 5s/epoch - 7ms/step\n",
      "Epoch 152/200\n",
      "674/674 - 5s - loss: 0.0427 - val_loss: 0.0432 - 5s/epoch - 7ms/step\n",
      "Epoch 153/200\n",
      "674/674 - 5s - loss: 0.0427 - val_loss: 0.0430 - 5s/epoch - 7ms/step\n",
      "Epoch 154/200\n",
      "674/674 - 5s - loss: 0.0427 - val_loss: 0.0432 - 5s/epoch - 7ms/step\n",
      "Epoch 155/200\n",
      "674/674 - 5s - loss: 0.0427 - val_loss: 0.0430 - 5s/epoch - 7ms/step\n",
      "Epoch 156/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0434 - 5s/epoch - 7ms/step\n",
      "Epoch 157/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0431 - 5s/epoch - 7ms/step\n",
      "Epoch 158/200\n",
      "674/674 - 5s - loss: 0.0428 - val_loss: 0.0431 - 5s/epoch - 7ms/step\n",
      "Epoch 159/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0431 - 5s/epoch - 8ms/step\n",
      "Epoch 160/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0429 - 5s/epoch - 7ms/step\n",
      "Epoch 161/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0429 - 5s/epoch - 7ms/step\n",
      "Epoch 162/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0428 - 5s/epoch - 8ms/step\n",
      "Epoch 163/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0429 - 5s/epoch - 7ms/step\n",
      "Epoch 164/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0428 - 5s/epoch - 7ms/step\n",
      "Epoch 165/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0432 - 5s/epoch - 7ms/step\n",
      "Epoch 166/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0429 - 5s/epoch - 7ms/step\n",
      "Epoch 167/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0430 - 5s/epoch - 7ms/step\n",
      "Epoch 168/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0428 - 5s/epoch - 7ms/step\n",
      "Epoch 169/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0430 - 5s/epoch - 7ms/step\n",
      "Epoch 170/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0430 - 5s/epoch - 7ms/step\n",
      "Epoch 171/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0431 - 5s/epoch - 8ms/step\n",
      "Epoch 172/200\n",
      "674/674 - 5s - loss: 0.0424 - val_loss: 0.0429 - 5s/epoch - 7ms/step\n",
      "Epoch 173/200\n",
      "674/674 - 4s - loss: 0.0425 - val_loss: 0.0431 - 4s/epoch - 7ms/step\n",
      "Epoch 174/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0432 - 5s/epoch - 7ms/step\n",
      "Epoch 175/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0431 - 5s/epoch - 7ms/step\n",
      "Epoch 176/200\n",
      "674/674 - 5s - loss: 0.0424 - val_loss: 0.0432 - 5s/epoch - 7ms/step\n",
      "Epoch 177/200\n",
      "674/674 - 4s - loss: 0.0425 - val_loss: 0.0431 - 4s/epoch - 7ms/step\n",
      "Epoch 178/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0432 - 5s/epoch - 7ms/step\n",
      "Epoch 179/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0431 - 5s/epoch - 7ms/step\n",
      "Epoch 180/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0434 - 5s/epoch - 7ms/step\n",
      "Epoch 181/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0434 - 5s/epoch - 7ms/step\n",
      "Epoch 182/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0435 - 5s/epoch - 8ms/step\n",
      "Epoch 183/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0430 - 5s/epoch - 7ms/step\n",
      "Epoch 184/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0429 - 5s/epoch - 8ms/step\n",
      "Epoch 185/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 186/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0433 - 5s/epoch - 7ms/step\n",
      "Epoch 187/200\n",
      "674/674 - 4s - loss: 0.0426 - val_loss: 0.0432 - 4s/epoch - 7ms/step\n",
      "Epoch 188/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0432 - 5s/epoch - 7ms/step\n",
      "Epoch 189/200\n",
      "674/674 - 4s - loss: 0.0425 - val_loss: 0.0434 - 4s/epoch - 7ms/step\n",
      "Epoch 190/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0434 - 5s/epoch - 7ms/step\n",
      "Epoch 191/200\n",
      "674/674 - 4s - loss: 0.0425 - val_loss: 0.0433 - 4s/epoch - 7ms/step\n",
      "Epoch 192/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0435 - 5s/epoch - 7ms/step\n",
      "Epoch 193/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n",
      "Epoch 194/200\n",
      "674/674 - 5s - loss: 0.0424 - val_loss: 0.0435 - 5s/epoch - 7ms/step\n",
      "Epoch 195/200\n",
      "674/674 - 6s - loss: 0.0425 - val_loss: 0.0434 - 6s/epoch - 8ms/step\n",
      "Epoch 196/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0434 - 5s/epoch - 7ms/step\n",
      "Epoch 197/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0437 - 5s/epoch - 7ms/step\n",
      "Epoch 198/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0435 - 5s/epoch - 8ms/step\n",
      "Epoch 199/200\n",
      "674/674 - 5s - loss: 0.0426 - val_loss: 0.0436 - 5s/epoch - 7ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200\n",
      "674/674 - 5s - loss: 0.0425 - val_loss: 0.0437 - 5s/epoch - 7ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Define the model\n",
    "conv_model2 = Sequential()\n",
    "conv_model2.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "conv_model2.add(MaxPooling1D(pool_size=2))\n",
    "conv_model2.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "conv_model2.add(MaxPooling1D(pool_size=2))\n",
    "conv_model2.add(Flatten())\n",
    "conv_model2.add(Dense(100, activation='relu'))\n",
    "conv_model2.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "conv_model2.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# Fit the model to the data\n",
    "history = conv_model2.fit(X_train, y_train, epochs=200, batch_size=72, validation_data=(X_test, y_test), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cb5a9fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515/1515 [==============================] - 4s 2ms/step\n",
      "1010/1010 [==============================] - 2s 2ms/step\n",
      "Convolutional Neural Network Train accuracy: 0.3602555851602608\n",
      "Convolutional Neural Network Test accuracy: 0.33263842229571494\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Make predictions\n",
    "train_preds = conv_model2.predict(X_train)\n",
    "test_preds = conv_model2.predict(X_test)\n",
    "\n",
    "train_r2 = r2_score(y_train, train_preds)\n",
    "test_r2 = r2_score(y_test, test_preds)\n",
    "\n",
    "print(f\"Convolutional Neural Network Train accuracy: {train_r2}\")\n",
    "print(f\"Convolutional Neural Network Test accuracy: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921f18f",
   "metadata": {},
   "source": [
    "This CNN model had a training accuracy of 0.36 and a testing accuracy of 0.333. The previous convolutional model performed better than this one. The previous model contained a total of 5 layers whereas this one contained a total of 7 layers, the main difference being that the one before used one max pooling layer whereas this one used two. It's possible that the addition of the extra max pooling layer in this model led to a loss of important information or reduced the resolution of the features being learned, resulting in a lower accuracy. \n",
    "\n",
    "Overall, the use of convolutional and max pooling layers in predicting local marginal price with an accuracy of 0.46 in the first convolutional model is significant. These layers are designed to extract features from the input data, which can be very useful in time series data like local marginal price. The convolutional layers can detect important patterns in the time series, such as trends and seasonality, while the max pooling layers can help to reduce the dimensionality of the data and improve the model's ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90452b2a",
   "metadata": {},
   "source": [
    "#### Comparing Machine Learning Models with Artificial Intelligence Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "01dce976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Training Accuracy  Testing Accuracy\n",
      "Model                                                                   \n",
      "Linear Regression                                0.122             0.132\n",
      "Random Forest                                    0.454             0.374\n",
      "Decision Tree                                    0.457             0.368\n",
      "Decision Tree Feature Selection                  0.357             0.322\n",
      "Simple Neural Network (DS Notebook)              0.337             0.292\n",
      "Simple Neural Network (DL Notebook)              0.441             0.338\n",
      "RNN Model 1                                      0.386             0.383\n",
      "RNN Model 2                                      0.403             0.401\n",
      "LSTM Model                                       0.455             0.421\n",
      "CNN Model 1                                      0.457             0.425\n",
      "CNN Model 2                                      0.360             0.332\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a list of dictionaries containing the model names and accuracy values\n",
    "model_data = [\n",
    "    {'Model': 'Linear Regression', 'Training Accuracy': 0.122, 'Testing Accuracy': 0.132},\n",
    "    {'Model': 'Random Forest', 'Training Accuracy': 0.454, 'Testing Accuracy': 0.374},\n",
    "    {'Model': 'Decision Tree', 'Training Accuracy': 0.457, 'Testing Accuracy': 0.368},\n",
    "    {'Model': 'Decision Tree Feature Selection', 'Training Accuracy': 0.357, 'Testing Accuracy': 0.322},\n",
    "    {'Model': 'Simple Neural Network (DS Notebook)', 'Training Accuracy': 0.337, 'Testing Accuracy': 0.292},\n",
    "    {'Model': 'Simple Neural Network (DL Notebook)', 'Training Accuracy': 0.441, 'Testing Accuracy': 0.338},\n",
    "    {'Model': 'RNN Model 1', 'Training Accuracy': 0.386, 'Testing Accuracy': 0.383},\n",
    "    {'Model': 'RNN Model 2', 'Training Accuracy': 0.403, 'Testing Accuracy': 0.401},\n",
    "    {'Model': 'LSTM Model', 'Training Accuracy': 0.455, 'Testing Accuracy': 0.421},\n",
    "    {'Model': 'CNN Model 1', 'Training Accuracy': 0.457, 'Testing Accuracy': 0.425},\n",
    "    {'Model': 'CNN Model 2', 'Training Accuracy': 0.36, 'Testing Accuracy': 0.332}\n",
    "]\n",
    "\n",
    "# create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(model_data)\n",
    "\n",
    "# set the index to be the Model column\n",
    "df.set_index('Model', inplace=True)\n",
    "\n",
    "# display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db699b6",
   "metadata": {},
   "source": [
    "The machine learning models and the neural network models used in both notebooks are suitable for time-series analysis. \n",
    "\n",
    "Machine learning algorithms typically involve feature engineering, where the user selects and transforms relevant variables to create input features for the model. These features are then used to train a machine learning model to predict future values of the time series.\n",
    "\n",
    "Neural networks can perform feature extraction automatically from the raw time series data without the need for explicit feature engineering. This can be particularly useful when dealing with complex time series data that may have non-linear relationships between the input and output variables.\n",
    "\n",
    "From the table above, the model with the best training and testing accuracy is very close between the LSTM and the convolutional neural networks for predicting the Local Marginal Price. \n",
    "\n",
    "Some of the machine learning models have training and testing accuracies that are similiar to that of the LSTM or CNN models like the decision tree model for example. \n",
    "\n",
    "The effectiveness of the LSTM and CNN models suggests that both LSTM and CNN models are effective in capturing the patterns and dependencies in the time series data for predicting the Local Marginal Price. The LSTM model is specifically designed for handling sequential data and can remember long-term dependencies in the data, while the CNN model is able to extract features and patterns from the data using its convolutional and pooling layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2274a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
